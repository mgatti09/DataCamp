{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Four. Defining neural networks with Keras\n",
    "\n",
    "In the final chapter, you'll use high-level APIs in TensorFlow to train a sign language letter classifier. You will use both the sequential and functional Keras APIs to train, validate, and evaluate models. You will also learn how to use the Estimators API to streamline the model definition and training process and to avoid errors.\n",
    "\n",
    "> **Topics:**\n",
    "- 1. Defining neural networks with Keras\n",
    "    - 1.1 The sequential model in Keras\n",
    "    - 1.2 Compiling a sequential model\n",
    "    - 1.3 Defining a multiple input model\n",
    "- 2. Training and validation with Keras\n",
    "    - 2.1. Training with Keras\n",
    "    - 2.2. Metrics and validation with Keras\n",
    "    - 2.3 Overfitting detection\n",
    "    - 2.4 Evaluating models\n",
    "- 3. Training models with the Estimators API\n",
    "    - 3.1. Preparing to train with Estimators\n",
    "    - 3.2. Defining Estimators\n",
    "- 4. Congratulations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras, Variable, ones, matmul\n",
    "\n",
    "filepath = '../_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Neural Networks with Keras\n",
    "\n",
    "### Classifying sign language letters\n",
    "\n",
    "![][01-sign_language_letters]\n",
    "\n",
    "### The sequential API\n",
    "\n",
    "![][02-sequential_API]\n",
    "\n",
    "- Input layer\n",
    "- Hidden layers\n",
    "- Output layer\n",
    "- Ordered in sequence\n",
    "\n",
    "### Building a sequential model\n",
    "```Python\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define first hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(28*28,)))\n",
    "\n",
    "# Define second hidden layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "### The functional API\n",
    "\n",
    "![][03-functional_API]\n",
    "\n",
    "### Using the functional API\n",
    "```Python\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define model 1 input layer shape\n",
    "model1_inputs = tf.keras.Input(shape=(28*28,))\n",
    "\n",
    "# Define model 2 input layer shape\n",
    "model2_inputs = tf.keras.Input(shape=(10,))\n",
    "\n",
    "# Define layer 1 for model 1\n",
    "model1_layer1 = tf.keras.layers.Dense(12, activation='relu')(model1_inputs)\n",
    "\n",
    "# Define layer 2 for model 1\n",
    "model1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model1_layer1)\n",
    "\n",
    "# Define layer 1 for model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(8, activation='relu')(model2_inputs)\n",
    "\n",
    "# Define layer 2 for model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model2_layer1)\n",
    "\n",
    "# Merge model 1 and model 2\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])\n",
    "\n",
    "# Define a functional model\n",
    "model = tf.keras.Model(inputs=[model1_inputs, model2_inputs], outputs=merged)\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "```\n",
    "\n",
    "[01-sign_language_letters]:_Docs/01-sign_language_letters.png\n",
    "[02-sequential_API]:_Docs/02-sequential_API.png\n",
    "[03-functional_API]:_Docs/03-functional_API.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The sequential model in Keras\n",
    "In chapter 3, we used components of the `keras` API in `tensorflow` to define a neural network, but we stopped short of using its full capabilities to streamline model definition and training. In this exercise, you will use the `keras` sequential model API to define a neural network that can be used to classify images of sign language letters. You will also use the `.summary()` method to print the model's architecture, including the shape and number of parameters associated with each layer.\n",
    "\n",
    "Note that the images were reshaped from (28, 28) to (784,), so that they could be used as inputs to a dense layer. Additionally, note that `keras` has been imported from `tensorflow` for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 12,732\n",
      "Trainable params: 12,732\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've defined a model, but we haven't compiled it. ***The compilation step in `keras` allows us to set the optimizer, loss function, and other useful training parameters in a single line of code***. Furthermore, the `.summary()` method allows us to view the model's architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Compiling a sequential model\n",
    "In this exercise, you will work towards classifying letters from the Sign Language MNIST dataset; however, you will adopt a different network architecture than what you used in the previous exercise. There will be fewer layers, but more nodes. Additionally, you will compile the model to use the `adam` optimizer and the `categorical_crossentropy` loss. You will also use a method in `keras` to summarize your model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 68        \n",
      "=================================================================\n",
      "Total params: 12,628\n",
      "Trainable params: 12,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Defining a multiple input model\n",
    "In some cases, the **sequential API** will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the **functional API** instead. ***If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this***. In this exercise, we will see how to do this. We will also use the `.summary()` method to examine the joint model's architecture.\n",
    "\n",
    "Note that `keras` has been imported from `tensorflow` for you. Additionally, the input layers of the first and second models have been defined as `m1_inputs` and `m2_inputs`, respectively. Note that the two models have the same architecture, but one of them uses a `sigmoid` activation in the first layer and the other uses a `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(None, 784), dtype=float32)\n",
      "Tensor(\"input_2:0\", shape=(None, 784), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m1_inputs = tf.keras.layers.Input(shape=(28*28,))\n",
    "m2_inputs = tf.keras.layers.Input(shape=(28*28,))\n",
    "\n",
    "print(m1_inputs)\n",
    "print(m2_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 12)           9420        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 12)           9420        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 4)            52          dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 4)            52          dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 4)            0           dense_6[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 18,944\n",
      "Trainable params: 18,944\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `.summary()` method yields a new column: `connected to`. This column tells you how layers connect to each other within the network. We can see that `dense_2`, for instance, is connected to the `input_2` layer. We can also see that the `add` layer, which merged the two models, connected to both `dense_1` and `dense_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and validation with Keras\n",
    "\n",
    "### Overview of training and evaluation\n",
    "1. Load and clean data\n",
    "2. Define model\n",
    "3. Train and validate model\n",
    "4. Evaluate model\n",
    "\n",
    "### How to train a model\n",
    "\n",
    "```Python\n",
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.fit(image_features, image_labels)\n",
    "```\n",
    "\n",
    "### The fit() operation\n",
    "- Required arguments\n",
    "    - `features`\n",
    "    - `labels`\n",
    "- Many optional arguments\n",
    "    - `batch_size`\n",
    "    - `epochs`\n",
    "    - `validation_split`\n",
    "\n",
    "### Batch size and epochs\n",
    "\n",
    "- The numbers of examples in each batch is the **batch size**.\n",
    "- The number of times you train on the full set of batches is called **numbers of epochs**\n",
    "- In the image the batch size is 5 and the number of epochs is 2.\n",
    "\n",
    "![][04-Batches_epochs]\n",
    "\n",
    "### Performing validation\n",
    "\n",
    "- The `validation_split` parameter it divide the data in two parts. \n",
    "    - The first part is the train set\n",
    "    - The second part is the validation set\n",
    "- Defining `validation_split = 0.20` means 20% of the data will be for validation   \n",
    "\n",
    "![][05-validation]\n",
    "\n",
    "```Python\n",
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)\n",
    "```\n",
    "\n",
    "- In the next image we can see the training loss and the evaluation loss separately.\n",
    "- If the training loss becomes substantially lower than the evaluation loss, is a clear indication the model is **overfitting**. To avoid overfittig we could: \n",
    "    - Terminate the training process before that point or\n",
    "    - add regularization or\n",
    "    - dropout    \n",
    "\n",
    "![][06-validation]\n",
    "\n",
    "### Changing the metric\n",
    "```Python\n",
    "# Recomile the model with the accuracy metric\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)\n",
    "```\n",
    "\n",
    "![][07-metric]\n",
    "\n",
    "### The evaluation() operation\n",
    "\n",
    "- It's always a good idea to split off a test set before you begin to train and validate, **this way you can check the performance on the test set and the end of the training process**.\n",
    "- Since you may tune model parameters in response to validation set performance, **using a separate test set will provide you with further assurance that you haven't overfitted**.\n",
    "\n",
    "![][08-evaluation]\n",
    "\n",
    "```Python\n",
    "# Evaluate the test set\n",
    "model.evaluate(test)\n",
    "```\n",
    "\n",
    "[04-Batches_epochs]:_Docs/04-Batches_epochs.png\n",
    "[05-validation]:_Docs/05-validation.png\n",
    "[06-validation]:_Docs/06-validation.png\n",
    "[07-metric]:_Docs/07-metric.png\n",
    "[08-evaluation]:_Docs/08-evaluation.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Training with Keras\n",
    "In this exercise, we return to our sign language letter classification problem. We have 2000 images of four letters--A, B, C, and D--and we want to classify them with a high level of accuracy. We will complete all parts of the problem, including the model definition, compilation, and training.\n",
    "\n",
    "Note that `keras` has been imported from `tensorflow` for you. Additionally, the features are available as `sign_language_features` and the targets are available as `sign_language_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = filepath+'slmnist.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>142</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>148</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>63</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>77</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>142</td>\n",
       "      <td>144</td>\n",
       "      <td>145</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>152</td>\n",
       "      <td>...</td>\n",
       "      <td>173</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>160</td>\n",
       "      <td>162</td>\n",
       "      <td>164</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>171</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>181</td>\n",
       "      <td>197</td>\n",
       "      <td>195</td>\n",
       "      <td>193</td>\n",
       "      <td>193</td>\n",
       "      <td>191</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "      <td>193</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>65</td>\n",
       "      <td>86</td>\n",
       "      <td>97</td>\n",
       "      <td>106</td>\n",
       "      <td>117</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>...</td>\n",
       "      <td>175</td>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>183</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>164</td>\n",
       "      <td>168</td>\n",
       "      <td>172</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>180</td>\n",
       "      <td>182</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>107</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>111</td>\n",
       "      <td>108</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0    1  142  143  146  148  149  149  149  150  151  ...    0   15   55   63   \n",
       "1    0  141  142  144  145  147  149  150  151  152  ...  173  179  179  180   \n",
       "2    1  156  157  160  162  164  166  169  171  171  ...  181  197  195  193   \n",
       "3    3   63   26   65   86   97  106  117  123  128  ...  175  179  180  182   \n",
       "4    1  156  160  164  168  172  175  178  180  182  ...  108  107  106  110   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0   37   61   77   65   38   23  \n",
       "1  181  181  182  182  183  183  \n",
       "2  193  191  192  198  193  182  \n",
       "3  183  183  184  185  185  185  \n",
       "4  111  108  108  102   84   70  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slmnist = pd.read_csv(file, header=None)\n",
    "df_slmnist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "sign_language_labels = df_slmnist[df_slmnist.columns[0]].values.reshape(-1,1)\n",
    "\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
    "sign_language_labels = onehot_encoder.fit_transform(target)\n",
    "sign_language_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting scaler\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "# Selecting features and applying scaler\n",
    "sign_language_features = max_abs_scaler.fit_transform(df_slmnist[df_slmnist.columns[1:]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 0s 83us/sample - loss: 1.2737\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 1.0011\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.7938\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.6345\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 0s 40us/sample - loss: 0.5231\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 0s 41us/sample - loss: 0.4382\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 0s 48us/sample - loss: 0.3793\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 0s 42us/sample - loss: 0.3322\n",
      "Epoch 9/10\n",
      "2000/2000 [==============================] - 0s 44us/sample - loss: 0.2897\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 0s 52us/sample - loss: 0.2536\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e39c30abe0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metrics and validation with Keras\n",
    "We trained a model to predict sign language letters in the previous exercise, but it is unclear how successful we were in doing so. In this exercise, we will try to improve upon the interpretability of our results. Since **we did not use a validation split, we only observed performance improvements within the training set; however, it is unclear how much of that was due to overfitting**. Furthermore, since we did not supply a metric, **we only saw decreases in the loss function, which do not have any clear interpretation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "1800/1800 [==============================] - 0s 196us/sample - loss: 1.1571 - accuracy: 0.5639 - val_loss: 0.9537 - val_accuracy: 0.7400\n",
      "Epoch 2/10\n",
      "1800/1800 [==============================] - 0s 58us/sample - loss: 0.7983 - accuracy: 0.7883 - val_loss: 0.7517 - val_accuracy: 0.7050\n",
      "Epoch 3/10\n",
      "1800/1800 [==============================] - 0s 76us/sample - loss: 0.5918 - accuracy: 0.8750 - val_loss: 0.5454 - val_accuracy: 0.8600\n",
      "Epoch 4/10\n",
      "1800/1800 [==============================] - 0s 54us/sample - loss: 0.4565 - accuracy: 0.9161 - val_loss: 0.5399 - val_accuracy: 0.6700\n",
      "Epoch 5/10\n",
      "1800/1800 [==============================] - 0s 50us/sample - loss: 0.3679 - accuracy: 0.9317 - val_loss: 0.3156 - val_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "1800/1800 [==============================] - 0s 69us/sample - loss: 0.2954 - accuracy: 0.9511 - val_loss: 0.2417 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "1800/1800 [==============================] - 0s 54us/sample - loss: 0.2378 - accuracy: 0.9617 - val_loss: 0.2652 - val_accuracy: 0.8900\n",
      "Epoch 8/10\n",
      "1800/1800 [==============================] - 0s 51us/sample - loss: 0.1968 - accuracy: 0.9678 - val_loss: 0.1661 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "1800/1800 [==============================] - 0s 57us/sample - loss: 0.1612 - accuracy: 0.9778 - val_loss: 0.1317 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "1800/1800 [==============================] - 0s 51us/sample - loss: 0.1367 - accuracy: 0.9811 - val_loss: 0.1101 - val_accuracy: 0.9950\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3b5524400>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(sign_language_features.shape[1],)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `keras` API, you only needed 14 lines of code to define, compile, train, and validate a model. You may have noticed that your model performed quite well. In just 10 epochs, we achieved a classification accuracy of around 99% in the validation sample!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Overfitting detection\n",
    "In this exercise, we'll work with a **small subset of the examples (50) from the original sign language letters dataset**. ***A small sample, coupled with a heavily-parameterized model, will generally lead to overfitting***. This means that your model will simply memorize the class of each example, rather than identifying features that generalize to many examples.\n",
    "\n",
    "You will detect overfitting by checking whether the validation sample loss is substantially higher than the training sample loss and whether it increases with further training. With a small sample and a high learning rate, the model will struggle to converge on an optimum. You will set a low learning rate for the optimizer, which will make it easier to identify overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random list for selecting a subsample of 50 examples\n",
    "import random\n",
    "l_index = random.sample(range(0,200), 50)\n",
    "\n",
    "subsample_sign_language_labels = sign_language_labels[l_index]\n",
    "subsample_sign_language_features = sign_language_features[l_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25 samples, validate on 25 samples\n",
      "Epoch 1/200\n",
      "25/25 [==============================] - 0s 4ms/sample - loss: 1.5293 - accuracy: 0.1600 - val_loss: 2.9971 - val_accuracy: 0.3200\n",
      "Epoch 2/200\n",
      "25/25 [==============================] - 0s 380us/sample - loss: 2.2117 - accuracy: 0.4400 - val_loss: 4.5554 - val_accuracy: 0.2400\n",
      "Epoch 3/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 3.6060 - accuracy: 0.2800 - val_loss: 3.2465 - val_accuracy: 0.2000\n",
      "Epoch 4/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 2.2808 - accuracy: 0.3200 - val_loss: 3.2402 - val_accuracy: 0.3200\n",
      "Epoch 5/200\n",
      "25/25 [==============================] - 0s 400us/sample - loss: 2.4964 - accuracy: 0.4000 - val_loss: 2.6508 - val_accuracy: 0.2000\n",
      "Epoch 6/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 1.9369 - accuracy: 0.3200 - val_loss: 1.6475 - val_accuracy: 0.2400\n",
      "Epoch 7/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 1.0717 - accuracy: 0.6000 - val_loss: 1.3201 - val_accuracy: 0.3200\n",
      "Epoch 8/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.9980 - accuracy: 0.4400 - val_loss: 1.3905 - val_accuracy: 0.4400\n",
      "Epoch 9/200\n",
      "25/25 [==============================] - 0s 362us/sample - loss: 1.2544 - accuracy: 0.4800 - val_loss: 1.4671 - val_accuracy: 0.2800\n",
      "Epoch 10/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 1.3697 - accuracy: 0.3200 - val_loss: 1.1498 - val_accuracy: 0.3600\n",
      "Epoch 11/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.9735 - accuracy: 0.5200 - val_loss: 1.0140 - val_accuracy: 0.5600\n",
      "Epoch 12/200\n",
      "25/25 [==============================] - 0s 400us/sample - loss: 0.6806 - accuracy: 0.9200 - val_loss: 1.2179 - val_accuracy: 0.3600\n",
      "Epoch 13/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.6922 - accuracy: 0.8800 - val_loss: 1.4036 - val_accuracy: 0.4400\n",
      "Epoch 14/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.7292 - accuracy: 0.8400 - val_loss: 1.5035 - val_accuracy: 0.5600\n",
      "Epoch 15/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.7484 - accuracy: 0.9200 - val_loss: 1.5908 - val_accuracy: 0.5600\n",
      "Epoch 16/200\n",
      "25/25 [==============================] - 0s 381us/sample - loss: 0.7896 - accuracy: 0.8800 - val_loss: 1.6010 - val_accuracy: 0.6000\n",
      "Epoch 17/200\n",
      "25/25 [==============================] - 0s 433us/sample - loss: 0.7742 - accuracy: 0.8800 - val_loss: 1.5076 - val_accuracy: 0.6000\n",
      "Epoch 18/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.6833 - accuracy: 0.9200 - val_loss: 1.3871 - val_accuracy: 0.5200\n",
      "Epoch 19/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.5952 - accuracy: 0.8800 - val_loss: 1.2461 - val_accuracy: 0.5200\n",
      "Epoch 20/200\n",
      "25/25 [==============================] - 0s 443us/sample - loss: 0.5236 - accuracy: 0.9600 - val_loss: 1.0724 - val_accuracy: 0.5200\n",
      "Epoch 21/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.4499 - accuracy: 0.9600 - val_loss: 0.9137 - val_accuracy: 0.6400\n",
      "Epoch 22/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.3933 - accuracy: 0.9600 - val_loss: 0.8349 - val_accuracy: 0.6800\n",
      "Epoch 23/200\n",
      "25/25 [==============================] - 0s 442us/sample - loss: 0.3865 - accuracy: 1.0000 - val_loss: 0.8374 - val_accuracy: 0.8000\n",
      "Epoch 24/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.4232 - accuracy: 1.0000 - val_loss: 0.8438 - val_accuracy: 0.7600\n",
      "Epoch 25/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.4391 - accuracy: 0.9600 - val_loss: 0.8097 - val_accuracy: 0.8000\n",
      "Epoch 26/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.3981 - accuracy: 0.9600 - val_loss: 0.7783 - val_accuracy: 0.7200\n",
      "Epoch 27/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.3382 - accuracy: 1.0000 - val_loss: 0.7978 - val_accuracy: 0.7200\n",
      "Epoch 28/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.3013 - accuracy: 1.0000 - val_loss: 0.8583 - val_accuracy: 0.6400\n",
      "Epoch 29/200\n",
      "25/25 [==============================] - 0s 440us/sample - loss: 0.2886 - accuracy: 1.0000 - val_loss: 0.9220 - val_accuracy: 0.6000\n",
      "Epoch 30/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.2858 - accuracy: 0.9600 - val_loss: 0.9629 - val_accuracy: 0.6000\n",
      "Epoch 31/200\n",
      "25/25 [==============================] - 0s 503us/sample - loss: 0.2835 - accuracy: 0.9600 - val_loss: 0.9813 - val_accuracy: 0.6000\n",
      "Epoch 32/200\n",
      "25/25 [==============================] - 0s 503us/sample - loss: 0.2806 - accuracy: 0.9600 - val_loss: 0.9842 - val_accuracy: 0.6400\n",
      "Epoch 33/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.2763 - accuracy: 0.9600 - val_loss: 0.9687 - val_accuracy: 0.6400\n",
      "Epoch 34/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.2652 - accuracy: 0.9600 - val_loss: 0.9329 - val_accuracy: 0.6400\n",
      "Epoch 35/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.2466 - accuracy: 0.9600 - val_loss: 0.8835 - val_accuracy: 0.6800\n",
      "Epoch 36/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.2264 - accuracy: 0.9600 - val_loss: 0.8285 - val_accuracy: 0.7200\n",
      "Epoch 37/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.2093 - accuracy: 1.0000 - val_loss: 0.7739 - val_accuracy: 0.7200\n",
      "Epoch 38/200\n",
      "25/25 [==============================] - 0s 542us/sample - loss: 0.1970 - accuracy: 1.0000 - val_loss: 0.7273 - val_accuracy: 0.7600\n",
      "Epoch 39/200\n",
      "25/25 [==============================] - 0s 460us/sample - loss: 0.1898 - accuracy: 1.0000 - val_loss: 0.6943 - val_accuracy: 0.7600\n",
      "Epoch 40/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.1859 - accuracy: 1.0000 - val_loss: 0.6766 - val_accuracy: 0.8000\n",
      "Epoch 41/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.1837 - accuracy: 1.0000 - val_loss: 0.6691 - val_accuracy: 0.8000\n",
      "Epoch 42/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.1804 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 0.8000\n",
      "Epoch 43/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.1736 - accuracy: 1.0000 - val_loss: 0.6585 - val_accuracy: 0.8000\n",
      "Epoch 44/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.1638 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 0.8000\n",
      "Epoch 45/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.1534 - accuracy: 1.0000 - val_loss: 0.6689 - val_accuracy: 0.7600\n",
      "Epoch 46/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.1449 - accuracy: 1.0000 - val_loss: 0.6880 - val_accuracy: 0.7600\n",
      "Epoch 47/200\n",
      "25/25 [==============================] - 0s 642us/sample - loss: 0.1392 - accuracy: 1.0000 - val_loss: 0.7097 - val_accuracy: 0.7600\n",
      "Epoch 48/200\n",
      "25/25 [==============================] - 0s 682us/sample - loss: 0.1357 - accuracy: 1.0000 - val_loss: 0.7254 - val_accuracy: 0.7600\n",
      "Epoch 49/200\n",
      "25/25 [==============================] - 0s 562us/sample - loss: 0.1331 - accuracy: 1.0000 - val_loss: 0.7303 - val_accuracy: 0.7600\n",
      "Epoch 50/200\n",
      "25/25 [==============================] - 0s 742us/sample - loss: 0.1299 - accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.7600\n",
      "Epoch 51/200\n",
      "25/25 [==============================] - 0s 582us/sample - loss: 0.1258 - accuracy: 1.0000 - val_loss: 0.7142 - val_accuracy: 0.7600\n",
      "Epoch 52/200\n",
      "25/25 [==============================] - 0s 702us/sample - loss: 0.1212 - accuracy: 1.0000 - val_loss: 0.7002 - val_accuracy: 0.7600\n",
      "Epoch 53/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.1163 - accuracy: 1.0000 - val_loss: 0.6843 - val_accuracy: 0.7600\n",
      "Epoch 54/200\n",
      "25/25 [==============================] - 0s 541us/sample - loss: 0.1113 - accuracy: 1.0000 - val_loss: 0.6663 - val_accuracy: 0.7600\n",
      "Epoch 55/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.1067 - accuracy: 1.0000 - val_loss: 0.6468 - val_accuracy: 0.7600\n",
      "Epoch 56/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.1026 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0992 - accuracy: 1.0000 - val_loss: 0.6124 - val_accuracy: 0.8000\n",
      "Epoch 58/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0966 - accuracy: 1.0000 - val_loss: 0.6024 - val_accuracy: 0.8000\n",
      "Epoch 59/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0943 - accuracy: 1.0000 - val_loss: 0.5973 - val_accuracy: 0.8000\n",
      "Epoch 60/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0919 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 0.8000\n",
      "Epoch 61/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0893 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 0.8000\n",
      "Epoch 62/200\n",
      "25/25 [==============================] - 0s 461us/sample - loss: 0.0864 - accuracy: 1.0000 - val_loss: 0.5936 - val_accuracy: 0.8000\n",
      "Epoch 63/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.5937 - val_accuracy: 0.8000\n",
      "Epoch 64/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0804 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8000\n",
      "Epoch 65/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0778 - accuracy: 1.0000 - val_loss: 0.6002 - val_accuracy: 0.7600\n",
      "Epoch 66/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0756 - accuracy: 1.0000 - val_loss: 0.6061 - val_accuracy: 0.7600\n",
      "Epoch 67/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0735 - accuracy: 1.0000 - val_loss: 0.6117 - val_accuracy: 0.7600\n",
      "Epoch 68/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.6150 - val_accuracy: 0.7600\n",
      "Epoch 69/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0699 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.7600\n",
      "Epoch 70/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.6123 - val_accuracy: 0.7600\n",
      "Epoch 71/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0664 - accuracy: 1.0000 - val_loss: 0.6072 - val_accuracy: 0.7600\n",
      "Epoch 72/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0646 - accuracy: 1.0000 - val_loss: 0.6010 - val_accuracy: 0.7600\n",
      "Epoch 73/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0628 - accuracy: 1.0000 - val_loss: 0.5945 - val_accuracy: 0.7600\n",
      "Epoch 74/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0610 - accuracy: 1.0000 - val_loss: 0.5879 - val_accuracy: 0.8000\n",
      "Epoch 75/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0593 - accuracy: 1.0000 - val_loss: 0.5811 - val_accuracy: 0.8000\n",
      "Epoch 76/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0578 - accuracy: 1.0000 - val_loss: 0.5741 - val_accuracy: 0.8000\n",
      "Epoch 77/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.5674 - val_accuracy: 0.8000\n",
      "Epoch 78/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0551 - accuracy: 1.0000 - val_loss: 0.5615 - val_accuracy: 0.8000\n",
      "Epoch 79/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.5574 - val_accuracy: 0.8000\n",
      "Epoch 80/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0526 - accuracy: 1.0000 - val_loss: 0.5555 - val_accuracy: 0.8000\n",
      "Epoch 81/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0513 - accuracy: 1.0000 - val_loss: 0.5554 - val_accuracy: 0.8000\n",
      "Epoch 82/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0501 - accuracy: 1.0000 - val_loss: 0.5562 - val_accuracy: 0.8000\n",
      "Epoch 83/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0489 - accuracy: 1.0000 - val_loss: 0.5573 - val_accuracy: 0.8000\n",
      "Epoch 84/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.5583 - val_accuracy: 0.8000\n",
      "Epoch 85/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.8000\n",
      "Epoch 86/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0456 - accuracy: 1.0000 - val_loss: 0.5596 - val_accuracy: 0.8000\n",
      "Epoch 87/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0446 - accuracy: 1.0000 - val_loss: 0.5604 - val_accuracy: 0.8000\n",
      "Epoch 88/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.5612 - val_accuracy: 0.8000\n",
      "Epoch 89/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.8000\n",
      "Epoch 90/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0418 - accuracy: 1.0000 - val_loss: 0.5614 - val_accuracy: 0.8000\n",
      "Epoch 91/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.5604 - val_accuracy: 0.8000\n",
      "Epoch 92/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0401 - accuracy: 1.0000 - val_loss: 0.5585 - val_accuracy: 0.8000\n",
      "Epoch 93/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0392 - accuracy: 1.0000 - val_loss: 0.5555 - val_accuracy: 0.8000\n",
      "Epoch 94/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0384 - accuracy: 1.0000 - val_loss: 0.5521 - val_accuracy: 0.8000\n",
      "Epoch 95/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0376 - accuracy: 1.0000 - val_loss: 0.5488 - val_accuracy: 0.8000\n",
      "Epoch 96/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.5464 - val_accuracy: 0.8000\n",
      "Epoch 97/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0361 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8000\n",
      "Epoch 98/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.5431 - val_accuracy: 0.8000\n",
      "Epoch 99/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.5416 - val_accuracy: 0.8000\n",
      "Epoch 100/200\n",
      "25/25 [==============================] - 0s 442us/sample - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.5399 - val_accuracy: 0.8000\n",
      "Epoch 101/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0334 - accuracy: 1.0000 - val_loss: 0.5380 - val_accuracy: 0.8000\n",
      "Epoch 102/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.5365 - val_accuracy: 0.8000\n",
      "Epoch 103/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0321 - accuracy: 1.0000 - val_loss: 0.5355 - val_accuracy: 0.8000\n",
      "Epoch 104/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.5352 - val_accuracy: 0.8400\n",
      "Epoch 105/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0310 - accuracy: 1.0000 - val_loss: 0.5353 - val_accuracy: 0.8400\n",
      "Epoch 106/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0304 - accuracy: 1.0000 - val_loss: 0.5356 - val_accuracy: 0.8400\n",
      "Epoch 107/200\n",
      "25/25 [==============================] - 0s 522us/sample - loss: 0.0298 - accuracy: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.8400\n",
      "Epoch 108/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.0293 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.8400\n",
      "Epoch 109/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.8400\n",
      "Epoch 110/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0283 - accuracy: 1.0000 - val_loss: 0.5356 - val_accuracy: 0.8400\n",
      "Epoch 111/200\n",
      "25/25 [==============================] - 0s 562us/sample - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.5354 - val_accuracy: 0.8400\n",
      "Epoch 112/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0273 - accuracy: 1.0000 - val_loss: 0.5351 - val_accuracy: 0.8400\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 0s 502us/sample - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.5347 - val_accuracy: 0.8400\n",
      "Epoch 114/200\n",
      "25/25 [==============================] - 0s 502us/sample - loss: 0.0264 - accuracy: 1.0000 - val_loss: 0.5341 - val_accuracy: 0.8400\n",
      "Epoch 115/200\n",
      "25/25 [==============================] - 0s 442us/sample - loss: 0.0260 - accuracy: 1.0000 - val_loss: 0.5334 - val_accuracy: 0.8400\n",
      "Epoch 116/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0255 - accuracy: 1.0000 - val_loss: 0.5326 - val_accuracy: 0.8400\n",
      "Epoch 117/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.5316 - val_accuracy: 0.8400\n",
      "Epoch 118/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.5304 - val_accuracy: 0.8400\n",
      "Epoch 119/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.8400\n",
      "Epoch 120/200\n",
      "25/25 [==============================] - 0s 561us/sample - loss: 0.0239 - accuracy: 1.0000 - val_loss: 0.5281 - val_accuracy: 0.8400\n",
      "Epoch 121/200\n",
      "25/25 [==============================] - 0s 562us/sample - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.5270 - val_accuracy: 0.8400\n",
      "Epoch 122/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.5260 - val_accuracy: 0.8400\n",
      "Epoch 123/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.5253 - val_accuracy: 0.8400\n",
      "Epoch 124/200\n",
      "25/25 [==============================] - 0s 642us/sample - loss: 0.0225 - accuracy: 1.0000 - val_loss: 0.5248 - val_accuracy: 0.8400\n",
      "Epoch 125/200\n",
      "25/25 [==============================] - 0s 722us/sample - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.5243 - val_accuracy: 0.8400\n",
      "Epoch 126/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0218 - accuracy: 1.0000 - val_loss: 0.5237 - val_accuracy: 0.8400\n",
      "Epoch 127/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.5232 - val_accuracy: 0.8400\n",
      "Epoch 128/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8400\n",
      "Epoch 129/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.5222 - val_accuracy: 0.8400\n",
      "Epoch 130/200\n",
      "25/25 [==============================] - 0s 522us/sample - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.5220 - val_accuracy: 0.8400\n",
      "Epoch 131/200\n",
      "25/25 [==============================] - 0s 341us/sample - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.5218 - val_accuracy: 0.8400\n",
      "Epoch 132/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.5216 - val_accuracy: 0.8400\n",
      "Epoch 133/200\n",
      "25/25 [==============================] - 0s 359us/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.5214 - val_accuracy: 0.8400\n",
      "Epoch 134/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.5211 - val_accuracy: 0.8400\n",
      "Epoch 135/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.5208 - val_accuracy: 0.8400\n",
      "Epoch 136/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.5204 - val_accuracy: 0.8400\n",
      "Epoch 137/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.5200 - val_accuracy: 0.8400\n",
      "Epoch 138/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.5196 - val_accuracy: 0.8400\n",
      "Epoch 139/200\n",
      "25/25 [==============================] - 0s 482us/sample - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.5193 - val_accuracy: 0.8400\n",
      "Epoch 140/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 0.5189 - val_accuracy: 0.8400\n",
      "Epoch 141/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.5184 - val_accuracy: 0.8400\n",
      "Epoch 142/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.5180 - val_accuracy: 0.8400\n",
      "Epoch 143/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.5177 - val_accuracy: 0.8400\n",
      "Epoch 144/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.5173 - val_accuracy: 0.8400\n",
      "Epoch 145/200\n",
      "25/25 [==============================] - 0s 543us/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.5168 - val_accuracy: 0.8400\n",
      "Epoch 146/200\n",
      "25/25 [==============================] - 0s 582us/sample - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.5162 - val_accuracy: 0.8400\n",
      "Epoch 147/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.5158 - val_accuracy: 0.8400\n",
      "Epoch 148/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.5154 - val_accuracy: 0.8400\n",
      "Epoch 149/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.5150 - val_accuracy: 0.8400\n",
      "Epoch 150/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.5146 - val_accuracy: 0.8400\n",
      "Epoch 151/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0154 - accuracy: 1.0000 - val_loss: 0.5142 - val_accuracy: 0.8400\n",
      "Epoch 152/200\n",
      "25/25 [==============================] - 0s 381us/sample - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.5140 - val_accuracy: 0.8400\n",
      "Epoch 153/200\n",
      "25/25 [==============================] - 0s 467us/sample - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.8400\n",
      "Epoch 154/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.5134 - val_accuracy: 0.8400\n",
      "Epoch 155/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.5130 - val_accuracy: 0.8400\n",
      "Epoch 156/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.5128 - val_accuracy: 0.8400\n",
      "Epoch 157/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.5126 - val_accuracy: 0.8400\n",
      "Epoch 158/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.5123 - val_accuracy: 0.8400\n",
      "Epoch 159/200\n",
      "25/25 [==============================] - 0s 501us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.5120 - val_accuracy: 0.8400\n",
      "Epoch 160/200\n",
      "25/25 [==============================] - 0s 421us/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.5118 - val_accuracy: 0.8400\n",
      "Epoch 161/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.5116 - val_accuracy: 0.8800\n",
      "Epoch 162/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.5113 - val_accuracy: 0.8800\n",
      "Epoch 163/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.8800\n",
      "Epoch 164/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.5110 - val_accuracy: 0.8800\n",
      "Epoch 165/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.5107 - val_accuracy: 0.8800\n",
      "Epoch 166/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.8800\n",
      "Epoch 167/200\n",
      "25/25 [==============================] - 0s 461us/sample - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.5100 - val_accuracy: 0.8800\n",
      "Epoch 168/200\n",
      "25/25 [==============================] - 0s 482us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.5097 - val_accuracy: 0.8800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.5095 - val_accuracy: 0.8800\n",
      "Epoch 170/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.8800\n",
      "Epoch 171/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.5090 - val_accuracy: 0.8800\n",
      "Epoch 172/200\n",
      "25/25 [==============================] - 0s 321us/sample - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.5087 - val_accuracy: 0.8800\n",
      "Epoch 173/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.5085 - val_accuracy: 0.8800\n",
      "Epoch 174/200\n",
      "25/25 [==============================] - 0s 461us/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.5083 - val_accuracy: 0.8800\n",
      "Epoch 175/200\n",
      "25/25 [==============================] - 0s 501us/sample - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.5082 - val_accuracy: 0.8800\n",
      "Epoch 176/200\n",
      "25/25 [==============================] - 0s 482us/sample - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.5081 - val_accuracy: 0.8800\n",
      "Epoch 177/200\n",
      "25/25 [==============================] - 0s 441us/sample - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.5079 - val_accuracy: 0.8800\n",
      "Epoch 178/200\n",
      "25/25 [==============================] - 0s 361us/sample - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.5076 - val_accuracy: 0.8800\n",
      "Epoch 179/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8800\n",
      "Epoch 180/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.5070 - val_accuracy: 0.8800\n",
      "Epoch 181/200\n",
      "25/25 [==============================] - 0s 501us/sample - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.5067 - val_accuracy: 0.8800\n",
      "Epoch 182/200\n",
      "25/25 [==============================] - 0s 502us/sample - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.5064 - val_accuracy: 0.8800\n",
      "Epoch 183/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.5062 - val_accuracy: 0.8800\n",
      "Epoch 184/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.8800\n",
      "Epoch 185/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.5060 - val_accuracy: 0.8800\n",
      "Epoch 186/200\n",
      "25/25 [==============================] - 0s 481us/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 0.5060 - val_accuracy: 0.8800\n",
      "Epoch 187/200\n",
      "25/25 [==============================] - 0s 563us/sample - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 0.8800\n",
      "Epoch 188/200\n",
      "25/25 [==============================] - 0s 534us/sample - loss: 0.0101 - accuracy: 1.0000 - val_loss: 0.5056 - val_accuracy: 0.8800\n",
      "Epoch 189/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.5053 - val_accuracy: 0.8800\n",
      "Epoch 190/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0099 - accuracy: 1.0000 - val_loss: 0.5049 - val_accuracy: 0.8800\n",
      "Epoch 191/200\n",
      "25/25 [==============================] - 0s 562us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.5046 - val_accuracy: 0.8800\n",
      "Epoch 192/200\n",
      "25/25 [==============================] - 0s 642us/sample - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.5043 - val_accuracy: 0.8800\n",
      "Epoch 193/200\n",
      "25/25 [==============================] - 0s 562us/sample - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.8800\n",
      "Epoch 194/200\n",
      "25/25 [==============================] - 0s 682us/sample - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.8800\n",
      "Epoch 195/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.8800\n",
      "Epoch 196/200\n",
      "25/25 [==============================] - 0s 602us/sample - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.5041 - val_accuracy: 0.8800\n",
      "Epoch 197/200\n",
      "25/25 [==============================] - 0s 622us/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.5040 - val_accuracy: 0.8800\n",
      "Epoch 198/200\n",
      "25/25 [==============================] - 0s 501us/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.5038 - val_accuracy: 0.8800\n",
      "Epoch 199/200\n",
      "25/25 [==============================] - 0s 521us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5034 - val_accuracy: 0.8800\n",
      "Epoch 200/200\n",
      "25/25 [==============================] - 0s 401us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.5031 - val_accuracy: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e3cbc3dbe0>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(1024, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Finish the model compilation\n",
    "model.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Complete the model fit operation\n",
    "model.fit(subsample_sign_language_features, subsample_sign_language_labels, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the validation loss, `val_loss`, **was substantially higher than the `training loss`, loss.** Furthermore, if `val_loss` started to increase before the training process was terminated, then we may have overfitted. When this happens, you will want to **try decreasing the number of epochs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluating models\n",
    "Two models have been trained and are available: `large_model`, which has many parameters; and `small_model`, which has fewer parameters. Both models have been trained using `train_features` and `train_labels`, which are available to you. A separate test set, which consists of `test_features` and `test_labels`, is also available.\n",
    "\n",
    "Your goal is to evaluate relative model performance and also determine whether either model exhibits signs of overfitting. You will do this by evaluating `large_model` and `small_model` on both the train and test sets. For each model, you can do this by applying the `.evaluate(x, y)` method to compute the loss for features `x` and labels `y`. You will then compare the four losses generated.\n",
    "\n",
    "```Python\n",
    "# Evaluate the small model using the train data\n",
    "small_train = small_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the small model using the test data\n",
    "small_test = small_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Evaluate the large model using the train data\n",
    "large_train = large_model.evaluate(train_features, train_labels)\n",
    "\n",
    "# Evaluate the large model using the test data\n",
    "large_test = large_model.evaluate(test_features, test_labels)\n",
    "\n",
    "# Print losses\n",
    "print('\\n Small - Train: {}, Test: {}'.format(small_train, small_test))\n",
    "print('Large - Train: {}, Test: {}'.format(large_train, large_test))\n",
    "\n",
    "\n",
    "<script.py> output:\n",
    "    \n",
    " 32/100 [========>.....................] - ETA: 0s - loss: 1.0467\n",
    "100/100 [==============================] - 0s 360us/sample - loss: 1.0176\n",
    "    \n",
    " 32/100 [========>.....................] - ETA: 0s - loss: 1.0472\n",
    "100/100 [==============================] - 0s 54us/sample - loss: 1.0893\n",
    "    \n",
    " 32/100 [========>.....................] - ETA: 0s - loss: 0.0621\n",
    "100/100 [==============================] - 0s 372us/sample - loss: 0.0473\n",
    "    \n",
    " 32/100 [========>.....................] - ETA: 0s - loss: 0.1021\n",
    "100/100 [==============================] - 0s 55us/sample - loss: 0.2126\n",
    "    \n",
    "     Small - Train: 1.017621760368347, Test: 1.0893175601959229\n",
    "    Large - Train: 0.047317686378955844, Test: 0.21255494594573976\n",
    "```\n",
    "\n",
    "Notice that the gap between the test and train set losses is **substantially higher** for `large_model`, **suggesting that overfitting may be an issue**. Furthermore, both test and train set performance is better for `large_model`. **This suggests that we may want to use `large_model`, but reduce the number of training epochs**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training models with the Estimators API\n",
    "\n",
    "### What is the Estimators API?\n",
    "- High level submodule\n",
    "- Less flexible\n",
    "- Enforces best practices\n",
    "- Faster deployment\n",
    "- Many premade models\n",
    "\n",
    "![][09-estimators]\n",
    "\n",
    "### Model specification and training\n",
    "\n",
    "1. Define feature columns\n",
    "2. Load and transform data\n",
    "3. Define an estimator\n",
    "4. Apply train operation\n",
    "\n",
    "### Defining feature columns\n",
    "\n",
    "```Python\n",
    "# Import tensorflow under its standard alias\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a numeric feature column\n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "\n",
    "# Define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\",[\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "\n",
    "# Create feature column list\n",
    "features_list = [size, rooms]\n",
    "\n",
    "# Define a matrix feature column\n",
    "features_list = [tf.feature_column.numeric_column('image', shape=(784,))]\n",
    "```\n",
    "\n",
    "### Loading and transforming data\n",
    "\n",
    "```Python\n",
    "# Define input data function\n",
    "def input_fn():\n",
    "    # Define feature dictionary\n",
    "    features = {\"size\": [1340, 1690, 2720], \"rooms\": [1, 3, 4]}\n",
    "    \n",
    "    # Define labels\n",
    "    labels = [221900, 538000, 180000]\n",
    "    return features, labels\n",
    "```\n",
    "\n",
    "### Define and train a regression estimator\n",
    "```Python\n",
    "# Define a deep neural network regression\n",
    "model0 = tf.estimator.DNNRegressor(feature_columns=feature_list,hidden_units=[10, 6, 6, 3])\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn, steps=20)\n",
    "```\n",
    "\n",
    "### Define and train a deep neural network\n",
    "```Python\n",
    "# Define a deep neural network classifier\n",
    "model1 = tf.estimator.DNNClassifier(feature_columns=feature_list,hidden_units=[32, 16, 8], n_classes=4)\n",
    "\n",
    "# Train the classifier\n",
    "model1.train(input_fn, steps=20)\n",
    "```\n",
    "\n",
    "- https://www.tensorflow.org/guide/estimators\n",
    "\n",
    "[09-estimators]:_Docs/09-estimators.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preparing to train with Estimators\n",
    "For this exercise, we'll return to the King County housing transaction dataset from chapter 2. We will again develop and train a machine learning model to predict house prices; however, this time, we'll do it using the `estimator` API.\n",
    "\n",
    "Rather than completing everything in one step, we'll break this procedure down into parts. We'll begin by defining the feature columns and loading the data. In the next exercise, we'll define and train a premade estimator. Note that `feature_column` has been imported for you from `tensorflow`. Additionally, numpy has been imported as `np`, and the Kings County housing dataset is available as a pandas DataFrame: `housing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0           0     0  ...      7        1180              0   \n",
       "1      7242     2.0           0     0  ...      7        2170            400   \n",
       "2     10000     1.0           0     0  ...      6         770              0   \n",
       "3      5000     1.0           0     0  ...      7        1050            910   \n",
       "4      8080     1.0           0     0  ...      8        1680              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "\n",
       "   sqft_lot15  \n",
       "0        5650  \n",
       "1        7639  \n",
       "2        8062  \n",
       "3        5000  \n",
       "4        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = filepath+'kc_house_data.csv'\n",
    "housing = pd.read_csv(file)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = tf.feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = tf.feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing.price)\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Defining Estimators\n",
    "In the previous exercise, you defined a list of feature columns, `feature_list`, and a data input function, `input_fn()`. In this exercise, you will build on that work by defining an estimator that makes use of input data.\n",
    "\n",
    "1. Use a deep neural network regressor with 2 nodes in both the first and second hidden layers and 1 training step.\n",
    "2. Modify the code to use a `LinearRegressor()`, remove the `hidden_units`, and set the number of steps to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0624 17:25:15.697633 14436 estimator.py:1811] Using temporary folder as model directory: D:\\Usuarios\\marcgaso\\AppData\\Local\\Temp\\tmpuy9nestq\n",
      "W0624 17:25:15.720728 14436 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0624 17:25:16.398309 14436 deprecation.py:323] From D:\\Usuarios\\marcgaso\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_estimator\\python\\estimator\\head\\base_head.py:574: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0624 17:25:16.475466 14436 deprecation.py:506] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0624 17:25:17.481926 14436 estimator.py:1811] Using temporary folder as model directory: D:\\Usuarios\\marcgaso\\AppData\\Local\\Temp\\tmp89ttu6nd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.linear.LinearRegressorV2 at 0x1e3c9ef9e10>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)\n",
    "\n",
    "# Define the model and set the number of steps\n",
    "model = tf.estimator.LinearRegressor(feature_columns=feature_list)\n",
    "model.train(input_fn, steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you have other premade estimator options, such as `BoostedTreesRegressor()`, and can also create your own custom `estimators`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Congratulations!\n",
    "\n",
    "### What you learned\n",
    "- **Chapter 1**\n",
    "    - Low-level, basic, and advanced operations\n",
    "    - Graph-based computation\n",
    "    - Gradient computation and optimization\n",
    "- **Chapter 2**\n",
    "    - Data loading and transformation\n",
    "    - Predefined and custom loss functions\n",
    "    - Linear models and batch training\n",
    "- **Chapter 3**\n",
    "    - Dense neural network layers\n",
    "    - Activation functions\n",
    "    - Optimization algorithms\n",
    "    - Training neural networks\n",
    "- **Chapter 4**\n",
    "    - Neural networks in Keras\n",
    "    - Training and validation\n",
    "    - The Estimators API\n",
    "    \n",
    "### TensorFlow extensions\n",
    "- **TensorFlow Hub**\n",
    "    - Pretrained models\n",
    "    - Transfer learning\n",
    "- **TensorFlow Probability**\n",
    "    - More statistical distributions\n",
    "    - Trainable distributions\n",
    "    - Extended set of optimizers\n",
    "\n",
    "### TensorFlow 2.0\n",
    "- **TensorFlow 2.0**\n",
    "    - `eager_execution()`\n",
    "    - Tighter `keras` integration\n",
    "    - `Estimators`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
