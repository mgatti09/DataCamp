{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Three. Neural Networks in TensorFlow\n",
    "\n",
    "In this chapter, you'll learn how to predict credit card default using neural networks defined and trained in TensorFlow. You will define dense layers, apply activation functions, select an optimizer, and apply regularization to reduce overfitting. You will take advantage of TensorFlow's flexibility by using both lowlevel linear algebra and high-level Keras API operations to define and train models.\n",
    "\n",
    "> **Topics:**\n",
    "- 1. Dense layers\n",
    "    - 1.1 The linear algebra of dense layers\n",
    "    - 1.2 The low-level approach with multiple examples\n",
    "    - 1.3 Using the dense layer operation\n",
    "- 2. Activation functions\n",
    "    - 2.1. Binary classification problems\n",
    "    - 2.2. Multiclass classification problems\n",
    "- 3. Optimizers\n",
    "    - 3.1. The dangers of local minima\n",
    "    - 3.2. Avoiding local minima\n",
    "- 4. Training a network in TensorFlow\n",
    "    - 4.1. Initialization in TensorFlow\n",
    "    - 4.2. Training neural networks with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras, Variable, ones, matmul\n",
    "\n",
    "filepath = '../_datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dense layers\n",
    "\n",
    "### The linear regression model\n",
    "\n",
    "![][01-NN_LR]\n",
    "\n",
    "### What is a neural network?\n",
    "\n",
    "![][02-NN]\n",
    "\n",
    "![][03-NN]\n",
    "\n",
    "### A trivial dense layer\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input data\n",
    "inputs = tf.constant([[1, 35]])\n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "\n",
    "# Multiply inputs by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# Define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product)\n",
    "\n",
    "```\n",
    "\n",
    "### Defining a complete model\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input layer\n",
    "inputs = tf.constant(data, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "```\n",
    "\n",
    "### High-level versus low-level approach\n",
    "- High-level approach\n",
    "    - High-level API operations\n",
    "```Python    \n",
    "dense = keras.layers.Dense(10, activation='sigmoid')\n",
    "```\n",
    "\n",
    "- Low-level approach\n",
    "    - Linear-algebraic operations\n",
    "```Python    \n",
    "prod = matmul(inputs, weights)\n",
    "dense = keras.activations.sigmoid(prod)\n",
    "```\n",
    "\n",
    "\n",
    "[01-NN_LR]:_Docs/01-NN_LR.png\n",
    "[02-NN]:_Docs/02-NN.png\n",
    "[03-NN]:_Docs/03-NN.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The linear algebra of dense layers\n",
    "There are two ways to define a dense layer in `tensorflow`. The first involves the use of low-level, linear algebraic operations. The second makes use of high-level `keras` operations. In this exercise, we will use the first method to construct the network shown in the image below.\n",
    "\n",
    "![][04-3_2_1_network]\n",
    "\n",
    "The input layer contains 3 features -- education, marital status, and age -- which are available as `borrower_features`. The hidden layer contains 2 nodes and the output layer contains a single node.\n",
    "\n",
    "For each layer, you will take the previous layer as an input, initialize a set of weights, compute the product of the inputs and weights, and then apply an activation function. Note that `Variable()`, `ones()`, `matmul()`, and `keras()` have been imported from `tensorflow`.\n",
    "\n",
    "[04-3_2_1_network]:_Docs/04-3_2_1_network.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.array([[ 2.,  1., 24.]], dtype = np.float32())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " dense1's output shape: (1, 2)\n",
      "\n",
      " prediction: 0.8807970285415649\n",
      "\n",
      " actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1\n",
    "dense1 = keras.activations.sigmoid(product1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
    "\n",
    "# From previous step\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1)\n",
    "\n",
    "# Initialize weights2 as 2x1 variable of ones\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 The low-level approach with multiple examples\n",
    "In this exercise, we'll build further intuition for the low-level approach by constructing the first dense hidden layer for the case where we have multiple examples. We'll assume the model is trained and the first layer weights, `weights1`, is available. We'll then perform matrix multiplication of the `borrower_features` tensor by the `weights1` variable. Recall that the `borrower_features` tensor includes education, marital status, and age. Finally, we'll apply the sigmoid function to the elements of `products1`, yielding `dense1`.\n",
    "\n",
    "$\n",
    "products1 = \\begin{bmatrix} \n",
    "                3 & 2 & 23 \\\\ \n",
    "                2 & 1 & 24 \\\\ \n",
    "                1 & 1 & 49 \\\\ \n",
    "                1 & 1 & 49 \\\\ \n",
    "                2 & 1 & 29 \n",
    "             \\end{bmatrix} \n",
    "             \\begin{bmatrix} \n",
    "                 -0.6 & 0.6  \\\\ \n",
    "                  0.8 & -0.3 \\\\ \n",
    "                  -0.09 & -0.08 \n",
    "             \\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.array([[ 3.,  3., 23.],\n",
    "                              [ 2.,  1., 24.],\n",
    "                              [ 1.,  1., 49.],\n",
    "                              [ 1.,  1., 49.],\n",
    "                              [ 2.,  1., 29.]], dtype = np.float32())\n",
    "weights1 = np.array([[-0.6 ,  0.6 ],\n",
    "                     [ 0.8 , -0.3 ],\n",
    "                     [-0.09, -0.08]], dtype = np.float32())\n",
    "\n",
    "borrower_features = tf.constant(borrower_features)\n",
    "weights1 = tf.constant(weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of borrower_features:  (5, 3)\n",
      "\n",
      " shape of weights1:  (3, 2)\n",
      "\n",
      " shape of dense1:  (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1\n",
    "dense1 = keras.activations.sigmoid(products1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our input data, `borrower_features`, is 5x3 because it consists of 5 examples for 3 features. The shape of `weights1` is 3x2, as it was in the previous exercise, since it does not depend on the number of examples. Finally, `dense1` is 5x2, which means that we can multiply it by the following set of weights, `weights2`, which we defined to be 2x1 in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the dense layer operation\n",
    "We've now seen how to define dense layers in `tensorflow` using linear algebra. In this exercise, we'll skip the linear algebra and let `keras` work out the details. This will allow us to construct the network below, which has 2 hidden layers and 10 features, using less code than we needed for the network with 1 hidden layer and 3 features.\n",
    "\n",
    "![][05-10_7_3_1_network]\n",
    "\n",
    "To construct this network, we'll need to define three dense layers, each of which takes the previous layer as an input, multiplies it by weights, and applies an activation function. Note that input data has been defined and is available as a 100x10 tensor: `borrower_features`. Additionally, the `keras.layers` module is available.\n",
    "\n",
    "[05-10_7_3_1_network]:_Docs/05-10_7_3_1_network.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = np.array(pd.read_csv(filepath+'borrowed_features.csv', header=None), dtype = np.float32())\n",
    "borrower_features = tf.constant(borrower_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " shape of dense1:  (100, 7)\n",
      "\n",
      " shape of dense2:  (100, 3)\n",
      "\n",
      " shape of predictions:  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just 8 lines of code, you were able to define 2 dense hidden layers and an output layer. This is the advantage of using high-level operations in `tensorflow`. Note that each layer has 100 rows because the input data contains 100 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation functions\n",
    "\n",
    "### What is an activation function?\n",
    "- **Components of a typical hidden layer**\n",
    "    - **Linear:** Matrix multiplication\n",
    "    - **Nonlinear:** Activation function\n",
    "    \n",
    "### The sigmoid activation function\n",
    "- **Sigmoid activation function**\n",
    "    - Binary classification\n",
    "    - Low-level: `tf.keras.activations.sigmoid()`\n",
    "    - High-level: `sigmoid`\n",
    "    \n",
    "### The relu activation function\n",
    "- **ReLu activation function**\n",
    "    - Hidden layers\n",
    "    - Low-level: `tf.keras.activations.relu()`\n",
    "    - High-level: `relu`\n",
    "    \n",
    "### The softmax activation function\n",
    "- **Softmax activation function**\n",
    "    - Output layer (>2 classes)\n",
    "    - High-level: `tf.keras.activations.softmax()`\n",
    "    - Low-level: `softmax`\n",
    "    \n",
    "![][06-relu_sigmoid]\n",
    "\n",
    "### Activation functions in neural networks\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "# Define input layer\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16, activation='relu')(inputs)\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(8, activation='sigmoid')(dense1)\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(4, activation='softmax')(dense2)\n",
    "\n",
    "```\n",
    "[06-relu_sigmoid]:_Docs/06-relu_sigmoid.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Binary classification problems\n",
    "In this exercise, you will again make use of credit card data. The target variable, `default`, indicates whether a credit card holder defaults on her payment in the following period. Since there are only two options--default or not--this is a binary classification problem. While the dataset has many features, you will focus on just three: the size of the three latest credit card bills. Finally, you will compute predictions from your untrained network, `outputs`, and compare those the target variable, `default`.\n",
    "\n",
    "The tensor of features has been loaded and is available as `bill_amounts`. Additionally, the `constant()`, `float32`, and `keras.layers.Dense()` operations are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331.0</td>\n",
       "      <td>14948.0</td>\n",
       "      <td>15549.0</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314.0</td>\n",
       "      <td>28959.0</td>\n",
       "      <td>29547.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1069.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940.0</td>\n",
       "      <td>19146.0</td>\n",
       "      <td>19131.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>36681.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
       "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
       "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
       "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
       "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
       "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
       "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
       "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
       "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0       0.0       0.0       0.0                           1  \n",
       "1    1000.0       0.0    2000.0                           1  \n",
       "2    1000.0    1000.0    5000.0                           0  \n",
       "3    1100.0    1069.0    1000.0                           0  \n",
       "4    9000.0     689.0     679.0                           0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uci_credit_card = pd.read_csv(filepath+'uci_credit_card.csv')\n",
    "df_uci_credit_card.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = df_uci_credit_card['default.payment.next.month'].values.reshape(-1,1)\n",
    "bill_amounts = df_uci_credit_card[['BILL_AMT1','BILL_AMT2','BILL_AMT3']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from features\n",
    "inputs = tf.constant(bill_amounts, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the code several times, you'll notice that the errors change each time. This is because you're using an untrained model with randomly initialized parameters. Furthermore, the errors fall on the interval between -1 and 1 because `default` is a binary variable that takes on values of 0 and 1 and `outputs` is a probability between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multiclass classification problems\n",
    "In this exercise, we expand beyond binary classification to cover multiclass problems. A multiclass problem has targets that can take on three or more values. In the credit card dataset, the education variable can take on 6 different values, each corresponding to a different level of education. We will use that as our target in this exercise and will also expand the feature set from 3 to 10 columns.\n",
    "\n",
    "As in the previous problem, you will define an input layer, dense layers, and an output layer. You will also print the untrained model's predictions, which are probabilities assigned to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "borrower_features = df_uci_credit_card[['BILL_AMT1','BILL_AMT2','BILL_AMT3',\"BILL_AMT4\",\"BILL_AMT5\",\"PAY_AMT1\",\"PAY_AMT2\",\"PAY_AMT3\",\"PAY_AMT4\",\"PAY_AMT5\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03770198 0.13165766 0.22784594 0.18070209 0.16888447 0.25320792]\n",
      " [0.03395255 0.1424841  0.27360177 0.24699776 0.18191315 0.12105078]\n",
      " [0.0301488  0.13805786 0.27993178 0.245279   0.18558043 0.12100209]\n",
      " [0.03586568 0.12992041 0.26556113 0.20972696 0.14697164 0.21195407]\n",
      " [0.09635121 0.1684512  0.1920806  0.2175894  0.14326315 0.18226445]]\n"
     ]
    }
   ],
   "source": [
    "# Construct input layer from borrower features\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Optimizers\n",
    "\n",
    "### How to find a minimum\n",
    "\n",
    "![][07-local_minima_dots_4_10]\n",
    "\n",
    "![][08-optimizers]\n",
    "\n",
    "### The gradient descent optimizer\n",
    "- **Stochastic gradient descent (SGD) optimizer**\n",
    "    - `tf.keras.optimizers.SGD()`\n",
    "    - `learning_rate`\n",
    "\n",
    "### The RMS prop optimizer\n",
    "- **Root mean squared (RMS) propagation optimizer**\n",
    "    - Applies different learning rates to each feature\n",
    "    - `tf.keras.optimizers.RMSprop()`\n",
    "    - `learning_rate`\n",
    "    - `decay`\n",
    "\n",
    "### The adam optimizer\n",
    "- **Adaptive moment (adam) optimizer**\n",
    "    - `tf.keras.optimizers.Adam()`\n",
    "    - `learning_rate`\n",
    "    - `beta1`\n",
    "    - `beta2`\n",
    "\n",
    "### A complete example\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Compute the predicted values and loss\n",
    "def loss_function(weights):\n",
    "    product = tf.matmul(borrower_features, weights)\n",
    "    predictions = tf.keras.activations.sigmoid(product)\n",
    "    return tf.keras.losses.binary_crossentropy(default, predictions)\n",
    "\n",
    "# Minimize the loss function with adam\n",
    "opt = tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.8)\n",
    "opt.minimize(lambda: loss_function(weights), var_list=[weights])\n",
    "```\n",
    "[07-local_minima_dots_4_10]:_Docs/07-local_minima_dots_4_10.png\n",
    "[08-optimizers]:_Docs/08-optimizers.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The dangers of local minima\n",
    "Consider the plot of the following loss function, `loss_function()`, which contains a global minimum, marked by the dot on the right, and several local minima, including the one marked by the dot on the left.\n",
    "\n",
    "![][07-local_minima_dots_4_10]\n",
    "\n",
    "In this exercise, you will try to find the global minimum of `loss_function()` using `keras.optimizers.SGD()`. You will do this twice, each time with a different initial value of the input to `loss_function()`. First, you will use `x_1`, which is a variable with an initial value of 6.0. Second, you will use `x_2`, which is a variable with an initial value of 0.3. Note that `loss_function()` has been defined and is available.\n",
    "\n",
    "```Python\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,float32)\n",
    "x_2 = Variable(0.3,float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "\n",
    "<script.py> output:\n",
    "    4.3801394 0.42052683\n",
    "```\n",
    "\n",
    "Notice that we used the same optimizer and loss function, but two different initial values. When we started at 6.0 with `x_1`, we found the global minimum at 4.38, marked by the dot on the right. When we started at 0.3, we stopped around 0.42 with `x_2`, the local minimum marked by a dot on the far left.\n",
    "\n",
    "\n",
    "### 3.2 Avoiding local minima\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as `loss_function()`.\n",
    "\n",
    "![][07-local_minima_dots_4_10]\n",
    "\n",
    "Several optimizers in `tensorflow` have a momentum parameter, including `SGD` and `RMSprop`. You will make use of RMSprop in this exercise.\n",
    "\n",
    "```Python\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())\n",
    "\n",
    "<script.py> output:\n",
    "    4.3150263 0.4205261\n",
    "```\n",
    "\n",
    "Recall that the global minimum is approximately 4.38. Notice that `opt_1` built momentum, bringing `x_1` closer to the global minimum. To the contrary, `opt_2`, which had a momentum parameter of 0.0, got stuck in the local minimum on the left.\n",
    "\n",
    "\n",
    "[07-local_minima_dots_4_10]:_Docs/07-local_minima_dots_4_10.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a network in TensorFlow\n",
    "\n",
    "![][11-eggholder_function]\n",
    "\n",
    "Find the global minima!!!\n",
    "\n",
    "- How we can select initial values for x and y, the two inputs of the eggholder function??\n",
    "- What if we have a loss function that depends of hundred of variables?\n",
    "\n",
    "### Random initializers\n",
    "- **Often need to initialize thousands of variables**\n",
    "    - `tf.ones()` may perform poorly\n",
    "    - Tedious and difficult to initialize variables individually\n",
    "- **Alternatively, draw initial values from distribution**\n",
    "    - Random normal\n",
    "    - Uniform\n",
    "    - Glorot initializer\n",
    "    \n",
    "### Initializing variables in TensorFlow\n",
    "```Python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define 500x500 random normal variable\n",
    "weights = tf.Variable(tf.random.normal([500, 500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable\n",
    "weights = tf.Variable(tf.random.truncated_normal([500, 500]))\n",
    "\n",
    "# Define a dense layer with the default initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "# Define a dense layer with the zeros initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')\n",
    "```\n",
    "\n",
    "### Neural networks and overfitting\n",
    "\n",
    "![][09-overfitting]\n",
    "\n",
    "- Overfitting is specially problematic for Neural Networks, which containt many parameters and are quite good at memorization.\n",
    "\n",
    "### Applying dropout\n",
    "\n",
    "- A simple solution to the overfitting problem is to use dropout, an operation that will randomly drop nodes in a layer during the training process as shown on the rigth neural network.\n",
    "- This will force your network to develop more robust rules for classification, since it cannot rely on any particular nodes being passed to an activation function.\n",
    "\n",
    "![][10-dropout]\n",
    "\n",
    "### Implementing dropout in a network\n",
    "```Python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input data\n",
    "inputs = np.array(borrower_features, np.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(16, activation='relu')(dense1)\n",
    "\n",
    "# Apply dropout operation\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.layers.Dense(1, activation='sigmoid')(dropout1)\n",
    "\n",
    "```\n",
    "\n",
    "[09-overfitting]:_Docs/09-overfitting.png\n",
    "[10-dropout]:_Docs/10-dropout.png\n",
    "[11-eggholder_function]:_Docs/11-eggholder_function.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization in TensorFlow\n",
    "A good initialization can reduce the amount of time needed to find the global minimum. In this exercise, we will initialize weights and biases for a neural network that will be used to predict credit card default decisions. To build intuition, we will use the low-level, linear algebraic approach, rather than making use of convenience functions and high-level `keras` operations. We will also expand the set of input features from 3 to 23. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "weights1 = tf.Variable(tf.random.normal([23, 7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "bias1 = tf.Variable(tf.ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "weights2 = tf.Variable(tf.random.normal([7,1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "bias2 = tf.Variable(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training neural networks with TensorFlow\n",
    "In this exercise, you will train a neural network to predict whether a credit card holder will default. The features and targets you will use to train your network are available in the Python shell as `borrower_features` and `default`. You defined the weights and biases in the previous exercise.\n",
    "\n",
    "Note that `output_layer` is defined as $σ(layer1∗weights2+bias2)$, where $σ$ is the sigmoid activation, `layer1` is a tensor of nodes for the first hidden dense layer, `weight2` is a tensor of weights, and `bias2` is the bias tensor.\n",
    "\n",
    "The trainable variables are `weights1`, `bias1`, `weights2`, and `bias2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining label\n",
    "default = df_uci_credit_card['default.payment.next.month'].values.reshape(-1,1).astype(np.float32())\n",
    "default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02      , 1.        , 0.33333334, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.12      , 1.        , 0.33333334, ..., 0.00161031, 0.        ,\n",
       "        0.00378311],\n",
       "       [0.09      , 1.        , 0.33333334, ..., 0.00161031, 0.00234451,\n",
       "        0.00945777],\n",
       "       ...,\n",
       "       [0.03      , 0.5       , 0.33333334, ..., 0.00676329, 0.00468901,\n",
       "        0.00586382],\n",
       "       [0.08      , 0.5       , 0.5       , ..., 0.00310145, 0.12417444,\n",
       "        0.00341236],\n",
       "       [0.05      , 0.5       , 0.33333334, ..., 0.00161031, 0.00234451,\n",
       "        0.00189155]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting scaler\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "\n",
    "# Selecting features and applying scaler\n",
    "borrower_features = df_uci_credit_card.drop(['ID','default.payment.next.month'], axis = 1).values.astype(np.float32())\n",
    "borrower_features = max_abs_scaler.fit_transform(borrower_features)\n",
    "borrower_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 22:37:27.090510 24356 deprecation.py:323] From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.0169091e-01 -2.2284541e+00 -3.3028159e-01 -2.0048859e+00\n",
      "  -4.3291447e-01 -2.2897229e+00  3.8875139e-01]\n",
      " [-7.7550590e-01 -4.8801219e-01  2.2942179e-01  5.3426832e-01\n",
      "  -7.9427963e-01 -3.6275822e-01  1.4135907e+00]\n",
      " [ 6.5496072e-02 -2.2146351e+00 -3.9696997e-01 -1.6366566e+00\n",
      "  -1.0747992e+00  7.1611100e-01  7.8944489e-04]\n",
      " [-3.7804073e-01 -1.7501838e+00 -1.1841477e-01  2.1301384e+00\n",
      "   7.8164917e-01 -5.5147207e-01  5.3297257e-01]\n",
      " [-9.9887699e-02 -6.3926972e-02 -5.9422247e-02 -8.3798029e-02\n",
      "   8.3038813e-01 -6.8809366e-01 -5.5373991e-01]\n",
      " [-3.4567182e+00 -8.4834415e-01 -2.7652435e+00  1.5756580e+00\n",
      "   1.9418348e+00 -3.5993240e+00 -1.6360222e+00]\n",
      " [-2.9568233e+00 -1.6227802e+00 -8.5211337e-02 -1.8524590e-01\n",
      "   5.5522573e-01 -8.8754028e-01 -2.5353632e+00]\n",
      " [-9.4716591e-01  5.1678133e-01 -9.5865762e-01  7.5114298e-01\n",
      "   5.1255792e-01 -2.1275477e+00 -1.3426723e+00]\n",
      " [-1.4681213e+00 -8.4127659e-01 -1.3692361e+00 -9.9739438e-01\n",
      "   1.1932820e+00 -9.7684968e-01 -1.5188956e+00]\n",
      " [-1.5696169e+00  2.7871201e+00 -2.4646039e+00  6.8617702e-01\n",
      "   6.2228435e-01  4.2779320e-01 -8.9605176e-01]\n",
      " [-9.2190158e-01  2.7046916e-01 -1.3234316e+00  2.0008895e+00\n",
      "   1.5012692e+00 -2.1139556e-01 -1.9747937e+00]\n",
      " [-2.6499623e-01  1.8703535e-02  1.1206803e-01 -1.7474215e+00\n",
      "  -1.7962251e+00 -1.4297338e+00 -8.2162613e-01]\n",
      " [-3.9845310e-02 -1.5679730e+00 -8.1947654e-02 -1.8003579e+00\n",
      "  -1.5913866e+00 -4.1945988e-01  8.3808351e-01]\n",
      " [ 1.7548597e+00 -2.5140693e+00 -3.5696295e-01  7.6266062e-01\n",
      "  -9.7797358e-01  1.4648327e-01 -4.2048338e-01]\n",
      " [ 1.5559973e+00 -9.3090065e-02 -1.6470541e+00 -9.8570830e-01\n",
      "  -2.3716258e-01 -6.5641499e-01 -8.5396838e-01]\n",
      " [ 1.7409960e+00 -2.7718678e+00  1.0300795e+00 -1.3490456e+00\n",
      "  -1.0520191e+00  1.7974343e-02 -1.8000521e-01]\n",
      " [-6.2748373e-01 -8.1590462e-01 -1.5774531e+00 -1.1087799e+00\n",
      "  -1.2054112e+00 -3.9351058e-01 -4.0985700e-01]\n",
      " [ 1.0853653e+00 -3.6725488e-01  1.7627681e+00  1.3847630e-01\n",
      "  -1.3296304e+00 -9.0712541e-01  1.9897707e+00]\n",
      " [ 3.9163470e-01 -1.2256311e+00  5.9901017e-01 -9.9821484e-01\n",
      "  -1.6533605e+00  1.2385391e+00  8.3659077e-01]\n",
      " [ 1.1406794e+00 -2.8491753e-01 -7.7436632e-01 -1.2585942e+00\n",
      "  -1.1538258e+00  5.8088875e-01  3.3858235e+00]\n",
      " [ 2.6396232e+00 -1.4237239e+00  1.2415816e+00 -2.2224951e+00\n",
      "  -1.8849608e+00  5.2528197e-01  1.2972343e+00]\n",
      " [-1.0619263e-01 -5.1893663e-01  8.8367403e-01  1.4469764e+00\n",
      "  -2.2528641e+00  1.4539803e+00 -3.1819135e-01]\n",
      " [ 1.6416034e+00 -4.4673786e-01  2.0471942e+00 -8.3687223e-02\n",
      "  -2.1239686e+00  1.2003248e-01  1.2716898e-01]]\n"
     ]
    }
   ],
   "source": [
    "def loss_function(weights1, bias1, weights2, bias2, features, targets):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = tf.nn.relu(matmul(features, weights1) + bias1)\n",
    "    # Apply dropout\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\tlayer2 = tf.nn.sigmoid(matmul(dropout, weights2) + bias2)\n",
    "    # Pass targets and layers2 to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, layer2)\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "\n",
    "for j in range(0, 30000, 2000):\n",
    "\tfeatures, targets = borrower_features[j:j+2000, :], default[j:j+2000, :]\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(weights1, bias1, weights2, bias2, features, targets), var_list=[weights1, bias1, weights2, bias2])\n",
    "    \n",
    "print(weights1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the benefits of using `tensorflow` is that you have the option to customize models down to the linear algebraic-level, as we've shown in the last two exercises. If you look at `weights1`, which was printed to the console, you can see that the objects we're working with are simply tensors. In the following chapter, we'll see how you can make use of high level APIs to streamline the process for standard models that do not require customization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
