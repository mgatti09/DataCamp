{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter Four. Using text data to detect fraud\n",
    "In this final chapter, you will use text data, text mining and topic modeling to detect fraudulent behavior.\n",
    "\n",
    "> **Topics:**\n",
    "- 1. Using text data\n",
    "    - 1.1. Word search with dataframes\n",
    "    - 1.2. Using list of terms\n",
    "    - 1.3. Creating a flag\n",
    "- 2. Text mining to detect fraud\n",
    "    - 2.1. Removing stopwords\n",
    "    - 2.2. Cleaning text data\n",
    "- 3. Topic modeling on fraud\n",
    "    - 3.1. Create dictionary and corpus\n",
    "    - 3.2. LDA model\n",
    "- 4. Flagging fraud based on topics\n",
    "    - 4.1. Interpreting the topic model\n",
    "    - 4.2. Finding fraudsters based on topic\n",
    "- 5. Lesson 5: Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "filepath = '../_datasets/chapter_4/'\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using text data\n",
    "### You will often encounter text data during fraud detection\n",
    "Types of useful text data:\n",
    "1. Emails from employees and/or clients\n",
    "2. Transaction descriptions\n",
    "3. Employee notes\n",
    "4. Insurance claim form description box\n",
    "5. Recorded telephone conversations\n",
    "6. ...\n",
    "\n",
    "### Text mining techniques for fraud detection\n",
    "1. Word search\n",
    "2. Sentiment analysis\n",
    "3. Word frequencies and topic analysis\n",
    "4. Style\n",
    "\n",
    "### Word search for fraud detection\n",
    "Flagging suspicious words:\n",
    "1. Simple, straightforward and easy to explain\n",
    "2. Match results can be used as a filter on top of machine learning model\n",
    "3. Match results can be used as a feature in a machine learning model\n",
    "\n",
    "### Word counts to flag fraud with pandas\n",
    "```Python\n",
    "# Using a string operator to find words\n",
    "df['email_body'].str.contains('money laundering')\n",
    "\n",
    "# Select data that matches\n",
    "df.loc[df['email_body'].str.contains('money laundering', na=False)]\n",
    "\n",
    "# Create a list of words to search for\n",
    "list_of_words = ['police', 'money laundering']\n",
    "df.loc[df['email_body'].str.contains('|'.join(list_of_words), na=False)]\n",
    "\n",
    "# Create a fraud flag\n",
    "df['flag'] = np.where((df['email_body'].str.contains('|'.join(list_of_words)) == True), 1, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Word search with dataframes\n",
    "In this exercise you're going to work with text data, containing emails from Enron employees. The **Enron scandal** is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is **to find all emails** that mention specific words, such as \"sell enron stock\".\n",
    "\n",
    "By using string operations on dataframes, you can easily sift through messy email data and create flags based on word-hits. The Enron email data has been put into a dataframe called `df` so let's search for suspicious terms. Feel free to explore `df` in the Console before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;8345058.1075840404046.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>2002-01-29 23:20:55</td>\n",
       "      <td>INVESTools Advisory\\nA Free Digest of Trusted ...</td>\n",
       "      <td>investools advisory free digest trusted invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;1512159.1075863666797.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>2000-09-20 19:07:00</td>\n",
       "      <td>----- Forwarded by Richard B Sanders/HOU/ECT o...</td>\n",
       "      <td>forwarded richard b sanders hou ect pm justin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         From  \\\n",
       "Message-ID                                                                      \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>  ('advdfeedback@investools.com')   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>    ('richard.sanders@enron.com')   \n",
       "\n",
       "                                                                           To  \\\n",
       "Message-ID                                                                      \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>  ('advdfeedback@investools.com')   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>    ('richard.sanders@enron.com')   \n",
       "\n",
       "                                                             Date  \\\n",
       "Message-ID                                                          \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>  2002-01-29 23:20:55   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>  2000-09-20 19:07:00   \n",
       "\n",
       "                                                                                        content  \\\n",
       "Message-ID                                                                                        \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>  INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>  ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
       "\n",
       "                                                                                  clean_content  \n",
       "Message-ID                                                                                       \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>  investools advisory free digest trusted invest...  \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>  forwarded richard b sanders hou ect pm justin ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(filepath+\"enron_emails_clean.csv\", index_col='Message-ID')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;6336501.1075841154311.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('sarah.palmer@enron.com')</td>\n",
       "      <td>('sarah.palmer@enron.com')</td>\n",
       "      <td>2002-02-01 14:53:35</td>\n",
       "      <td>\\nJoint Venture: A 1997 Enron Meeting Belies O...</td>\n",
       "      <td>joint venture enron meeting belies officers cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    From  \\\n",
       "Message-ID                                                                 \n",
       "<6336501.1075841154311.JavaMail.evans@thyme>  ('sarah.palmer@enron.com')   \n",
       "\n",
       "                                                                      To  \\\n",
       "Message-ID                                                                 \n",
       "<6336501.1075841154311.JavaMail.evans@thyme>  ('sarah.palmer@enron.com')   \n",
       "\n",
       "                                                             Date  \\\n",
       "Message-ID                                                          \n",
       "<6336501.1075841154311.JavaMail.evans@thyme>  2002-02-01 14:53:35   \n",
       "\n",
       "                                                                                        content  \\\n",
       "Message-ID                                                                                        \n",
       "<6336501.1075841154311.JavaMail.evans@thyme>  \\nJoint Venture: A 1997 Enron Meeting Belies O...   \n",
       "\n",
       "                                                                                  clean_content  \n",
       "Message-ID                                                                                       \n",
       "<6336501.1075841154311.JavaMail.evans@thyme>  joint venture enron meeting belies officers cl...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all cleaned emails that contain 'sell enron stock'\n",
    "mask = df['clean_content'].str.contains('sell enron stock', na=False)\n",
    "\n",
    "# Select the data from df using the mask\n",
    "df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that searching for particular string values in a dataframe can be relatively easy, and allows you to include textual data into your model or analysis. You can use this word search as an additional flag, or as a feauture in your fraud detection model. Let's now have a look at how to filter the data using multiple search terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using list of terms\n",
    "Oftentimes you don't want to search on just one term. You probably can create a full **\"fraud dictionary\"** of terms that could potentially **flag fraudulent clients** and/or transactions. Fraud analysts often will have an idea what should be in such a dictionary. In this exercise you're going to **flag a multitude of terms**, and in the next exercise you'll create a new flag variable out of it. The 'flag' can be used either directly in a machine learning model as a feature, or as an additional filter on top of your machine learning model results. Let's first use a **list of terms** to filter our data on. The dataframe containing the cleaned emails is again available as `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>From</th>\n",
       "      <th>To</th>\n",
       "      <th>Date</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Message-ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;8345058.1075840404046.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>('advdfeedback@investools.com')</td>\n",
       "      <td>2002-01-29 23:20:55</td>\n",
       "      <td>INVESTools Advisory\\nA Free Digest of Trusted ...</td>\n",
       "      <td>investools advisory free digest trusted invest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;1512159.1075863666797.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>('richard.sanders@enron.com')</td>\n",
       "      <td>2000-09-20 19:07:00</td>\n",
       "      <td>----- Forwarded by Richard B Sanders/HOU/ECT o...</td>\n",
       "      <td>forwarded richard b sanders hou ect pm justin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;26118676.1075862176383.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>('m..love@enron.com')</td>\n",
       "      <td>2001-10-30 16:15:17</td>\n",
       "      <td>hey you are not wearing your target purple shi...</td>\n",
       "      <td>hey wearing target purple shirt today mine wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;10369289.1075860831062.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>('leslie.milosevich@kp.org')</td>\n",
       "      <td>2002-01-30 17:54:18</td>\n",
       "      <td>Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...</td>\n",
       "      <td>leslie milosevich santa clara avenue alameda c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;26728895.1075860815046.JavaMail.evans@thyme&gt;</th>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>('rtwait@graphicaljazz.com')</td>\n",
       "      <td>2002-01-30 19:36:01</td>\n",
       "      <td>Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...</td>\n",
       "      <td>rini twait e th ave longmont co rtwait graphic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          From  \\\n",
       "Message-ID                                                                       \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>   ('advdfeedback@investools.com')   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>     ('richard.sanders@enron.com')   \n",
       "<26118676.1075862176383.JavaMail.evans@thyme>            ('m..love@enron.com')   \n",
       "<10369289.1075860831062.JavaMail.evans@thyme>     ('leslie.milosevich@kp.org')   \n",
       "<26728895.1075860815046.JavaMail.evans@thyme>     ('rtwait@graphicaljazz.com')   \n",
       "\n",
       "                                                                            To  \\\n",
       "Message-ID                                                                       \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>   ('advdfeedback@investools.com')   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>     ('richard.sanders@enron.com')   \n",
       "<26118676.1075862176383.JavaMail.evans@thyme>            ('m..love@enron.com')   \n",
       "<10369289.1075860831062.JavaMail.evans@thyme>     ('leslie.milosevich@kp.org')   \n",
       "<26728895.1075860815046.JavaMail.evans@thyme>     ('rtwait@graphicaljazz.com')   \n",
       "\n",
       "                                                              Date  \\\n",
       "Message-ID                                                           \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>   2002-01-29 23:20:55   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>   2000-09-20 19:07:00   \n",
       "<26118676.1075862176383.JavaMail.evans@thyme>  2001-10-30 16:15:17   \n",
       "<10369289.1075860831062.JavaMail.evans@thyme>  2002-01-30 17:54:18   \n",
       "<26728895.1075860815046.JavaMail.evans@thyme>  2002-01-30 19:36:01   \n",
       "\n",
       "                                                                                         content  \\\n",
       "Message-ID                                                                                         \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>   INVESTools Advisory\\nA Free Digest of Trusted ...   \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>   ----- Forwarded by Richard B Sanders/HOU/ECT o...   \n",
       "<26118676.1075862176383.JavaMail.evans@thyme>  hey you are not wearing your target purple shi...   \n",
       "<10369289.1075860831062.JavaMail.evans@thyme>  Leslie Milosevich\\n1042 Santa Clara Avenue\\nAl...   \n",
       "<26728895.1075860815046.JavaMail.evans@thyme>  Rini Twait\\n1010 E 5th Ave\\nLongmont, CO 80501...   \n",
       "\n",
       "                                                                                   clean_content  \n",
       "Message-ID                                                                                        \n",
       "<8345058.1075840404046.JavaMail.evans@thyme>   investools advisory free digest trusted invest...  \n",
       "<1512159.1075863666797.JavaMail.evans@thyme>   forwarded richard b sanders hou ect pm justin ...  \n",
       "<26118676.1075862176383.JavaMail.evans@thyme>  hey wearing target purple shirt today mine wan...  \n",
       "<10369289.1075860831062.JavaMail.evans@thyme>  leslie milosevich santa clara avenue alameda c...  \n",
       "<26728895.1075860815046.JavaMail.evans@thyme>  rini twait e th ave longmont co rtwait graphic...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of terms to search for\n",
    "searchfor = [ 'enron stock', 'sell stock', 'stock bonus', 'sell enron stock']\n",
    "\n",
    "# Filter cleaned emails on searchfor list and select from df \n",
    "filtered_emails = df.loc[df['clean_content'].str.contains('|'.join(searchfor), na=False)]\n",
    "filtered_emails.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By joining the search terms with the 'or' sign, i.e. `|`, you can search on a multitude of terms in your dataset very easily. Let's now create a flag from this which you can use as a feature in a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating a flag\n",
    "This time you are going to **create an actual flag** variable that gives a **1 when the emails get a hit on the search terms of interest**, and 0 otherwise. This is the last step you need to make in order to actually use the text data content as a feature in a machine learning model, or as an actual flag on top of model results. You can continue working with the dataframe `df` containing the emails, and the `searchfor` list is the one defined in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1776\n",
       "1     314\n",
       "Name: flag, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create flag variable where the emails match the searchfor terms\n",
    "df['flag'] = np.where((df['clean_content'].str.contains('|'.join(searchfor)) == True), 1, 0)\n",
    "\n",
    "# Count the values of the flag variable\n",
    "count = df['flag'].value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now managed to search for a list of strings in several lines of text data. These skills come in handy when you want to flag certain words based on what you discovered in your topic model, or when you know beforehand what you want to search for. In the next exercises you're going to learn how to clean text data and to create your own topic model to further look for indications of fraud in your text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text mining techniques for fraud detection\n",
    "### Cleaning your text data\n",
    "Must do's when working with textual data:\n",
    "1. Tokenization\n",
    "2. Remove all stopwords\n",
    "3. Lemmatize your words\n",
    "4. Stem your words\n",
    "\n",
    "### Go from this...\n",
    "![][28-from]\n",
    "\n",
    "### To this...\n",
    "![][29-to]\n",
    "\n",
    "### Data preprocessing part 1\n",
    "```Python\n",
    "# 1. Tokenization\n",
    "from nltk import word_tokenize\n",
    "text = df.apply(lambda row: word_tokenize(row[\"email_body\"]), axis=1)\n",
    "text = text.rstrip()\n",
    "text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "# 2. Remove all stopwords and punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "stop = set(stopwords.words('english'))\n",
    "stop_free = \" \".join([word for word in text if((word not in stop) and (not word.isdigit()))])\n",
    "punc_free = ''.join(word for word in stop_free if word not in exclude)\n",
    "```\n",
    "\n",
    "### Data preprocessing part 2\n",
    "```Python\n",
    "# 3. Lemmatize words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "\n",
    "# 4. Stem words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter= PorterStemmer()\n",
    "cleaned_text = \" \".join(porter.stem(token) for token in normalized.split())\n",
    "print (cleaned_text)\n",
    "['philip','going','street','curious','hear','perspective','may','wish',\n",
    "'offer','trading','floor','enron','stock','lower','joined','company',\n",
    "'business','school','imagine','quite','happy','people','day','relate',\n",
    "'somewhat','stock','around','fact','broke','day','ago','knowing',\n",
    "'imagine','letting','event','get','much','taken','similar',\n",
    "'problem','hope','everything','else','going','well','family','knee',\n",
    "'surgery','yet','give','call','chance','later']\n",
    "```\n",
    "\n",
    "[28-from]:_Docs/28-from.png\n",
    "[29-to]:_Docs/29-to.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Removing stopwords\n",
    "In the following exercises you're going to **clean the Enron emails**, in order to be able to use the data in a topic model. Text cleaning can be challenging, so you'll learn some steps to do this well. The dataframe containing the emails `df` is available. In a first step you need to **define the list of stopwords and punctuations** that are to be removed in the next exercise from the text data. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk packages and string \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Define stopwords to exclude\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\", \"ect\", \"u\", \"fwd\", \"www\", \"com\"))\n",
    "\n",
    "# Define punctuations to exclude and lemmatizer\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cleaning text data\n",
    "Now that you've defined the **stopwords and punctuations**, let's use these to **clean our enron emails** in the dataframe `df` further. The lists containing stopwords and punctuations are available under `stop` and `exclude`. There are a few more steps to take before you have cleaned data, such as **\"lemmatization\" of words, and stemming the verbs**. The verbs in the email data are already stemmed, and the lemmatization is already done for you in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['investools', 'advisory', 'free', 'digest', 'trusted', 'investment', 'advice', 'unsubscribe', 'free', 'newsletter', 'please', 'see', 'issue', 'fried', 'sell', 'stock', 'gain', 'month', 'km', 'rowe', 'january', 'index', 'confirms', 'bull', 'market', 'aloy', 'small', 'cap', 'advisor', 'earns', 'lbix', 'compounding', 'return', 'pine', 'tree', 'pcl', 'undervalued', 'high', 'yield', 'bank', 'put', 'customer', 'first', 'aso', 'word', 'sponsor', 'top', 'wall', 'street', 'watcher', 'ben', 'zacks', 'year', 'year', 'gain', 'moving', 'best', 'brightest', 'wall', 'street', 'big', 'money', 'machine', 'earned', 'ben', 'zacks', 'five', 'year', 'average', 'annual', 'gain', 'start', 'outperforming', 'long', 'term', 'get', 'zacks', 'latest', 'stock', 'buylist', 'free', 'day', 'trial', 'investools', 'c', 'go', 'zaks', 'mtxtu', 'zakstb', 'investools', 'advisory', 'john', 'brobst', 'investools', 'fried', 'sell', 'stock', 'lock', 'month', 'km', 'david', 'fried', 'know', 'stock', 'undervalued', 'company', 'management', 'buy', 'back', 'share', 'open', 'market', 'latest', 'triumph', 'pocketing', 'impressive', 'gain', 'three', 'short', 'month', 'selling', 'four', 'buyback', 'stock', 'include', 'gain', 'auto', 'retailer', 'automation', 'incorporated', 'gain', 'digital', 'phone', 'system', 'purveyor', 'inter', 'tel', 'intl', 'fried', 'recent', 'move', 'buy', 'kmart', 'corporation', 'km', 'beleaguered', 'discount', 'retailer', 'declared', 'bankruptcy', 'think', 'k', 'mart', 'go', 'business', 'fried', 'say', 'take', 'recovery', 'possibility', 'bought', 'share', 'another', 'fried', 'pick', 'c', 'cor', 'net', 'corporation', 'ccbl', 'provides', 'range', 'technology', 'service', 'broadband', 'network', 'today', 'telecom', 'spending', 'slowdown', 'hit', 'company', 'hard', 'net', 'sale', 'fell', 'million', 'last', 'quarter', 'caused', 'net', 'loss', 'million', 'v', 'million', 'gain', 'last', 'year', 'fried', 'cite', 'buyback', 'plan', 'million', 'restructuring', 'charge', 'proof', 'management', 'see', 'rosier', 'future', 'david', 'fried', 'advice', 'see', 'buyback', 'index', 'portfolio', 'january', 'buyback', 'letter', 'david', 'fried', 'provides', 'wealth', 'building', 'opportunity', 'company', 'repurchasing', 'stock', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'back', 'mtxtu', 'back', 'rowe', 'january', 'index', 'confirms', 'bull', 'market', 'aloy', 'rowe', 'say', 'january', 'index', 'confirms', 'see', 'bull', 'market', 'first', 'five', 'trading', 'day', 'provided', 'gain', 'nasdaq', 'p', 'dow', 'industrials', 'rowe', 'say', 'five', 'day', 'index', 'correctly', 'predicted', 'market', 'direction', 'year', 'since', 'four', 'exception', 'include', 'three', 'war', 'year', 'fed', 'fund', 'rate', 'doubled', 'year', 'rowe', 'maintains', 'sure', 'recommendation', 'seven', 'company', 'say', 'leading', 'market', 'one', 'alloy', 'incorporated', 'aloy', 'medium', 'company', 'direct', 'marketer', 'provides', 'content', 'community', 'commerce', 'generation', 'roughly', 'million', 'people', 'year', 'age', 'rowe', 'like', 'market', 'account', 'billion', 'disposable', 'income', 'grow', 'faster', 'overall', 'population', 'q', 'saw', 'earnings', 'increase', 'sale', 'another', 'rowe', 'pick', 'new', 'century', 'financial', 'corporation', 'ncen', 'financier', 'make', 'buy', 'sell', 'service', 'sub', 'prime', 'mortgage', 'loan', 'secured', 'first', 'mortgage', 'single', 'family', 'home', 'borrower', 'typically', 'plenty', 'equity', 'property', 'secure', 'loan', 'suffer', 'weak', 'credit', 'profile', 'high', 'debt', 'income', 'ratio', 'q', 'earnings', 'grew', 'hike', 'sale', 'rowe', 'advice', 'see', 'investment', 'opportunity', 'february', 'wall', 'street', 'digest', 'momentum', 'investor', 'donald', 'rowe', 'target', 'stock', 'mutual', 'fund', 'capable', 'generating', 'annual', 'return', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'wall', 'mtxtu', 'wall', 'small', 'cap', 'advisor', 'earns', 'lbix', 'major', 'index', 'suffered', 'terrible', 'year', 'richard', 'geist', 'recommendation', 'earned', 'healthy', 'list', 'many', 'reason', 'selection', 'see', 'growth', 'going', 'forward', 'include', 'extremely', 'bullish', 'monetary', 'condition', 'high', 'productivity', 'inflation', 'sight', 'yield', 'curve', 'continues', 'steepen', 'also', 'investor', 'sentiment', 'poll', 'becoming', 'bearish', 'always', 'contrary', 'indicator', 'say', 'geist', 'latest', 'recommendation', 'buy', 'share', 'leading', 'brand', 'lbix', 'company', 'canada', 'largest', 'independent', 'food', 'brand', 'management', 'company', 'expanding', 'u', 'geist', 'particularly', 'like', 'firm', 'save', 'money', 'integrated', 'distribution', 'system', 'system', 'make', 'product', 'raw', 'material', 'provides', 'packaging', 'warehousing', 'distribution', 'recent', 'financial', 'result', 'show', 'leading', 'brand', 'roll', 'fy', 'saw', 'revenue', 'grow', 'million', 'net', 'income', 'million', 'per', 'share', 'last', 'year', 'loss', 'geist', 'predicts', 'company', 'see', 'revenue', 'reach', 'million', 'million', 'yield', 'forward', 'pe', 'think', 'lbix', 'significantly', 'undervalued', 'geist', 'say', 'range', 'leading', 'brand', 'strong', 'buy', 'richard', 'geist', 'advice', 'see', 'highlighted', 'stock', 'february', 'richard', 'geist', 'strategic', 'investing', 'richard', 'geist', 'integrates', 'psychological', 'aspect', 'investing', 'methodology', 'selecting', 'small', 'company', 'stock', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'stin', 'mtxtu', 'stin', 'compounding', 'return', 'pine', 'tree', 'pcl', 'growing', 'tree', 'usually', 'noisy', 'business', 'catch', 'attention', 'investment', 'medium', 'good', 'business', 'say', 'dick', 'young', 'timber', 'business', 'le', 'volatile', 'capital', 'intensive', 'manufacturing', 'young', 'see', 'demand', 'timber', 'increasing', 'population', 'increase', 'note', 'average', 'return', 'timber', 'investment', 'outperformed', 'p', 'average', 'annual', 'return', 'young', 'favorite', 'timber', 'play', 'plum', 'creek', 'timber', 'pcl', 'one', 'largest', 'private', 'timberland', 'owner', 'u', 'reit', 'primary', 'goal', 'profit', 'acquiring', 'managing', 'land', 'young', 'say', 'plum', 'creek', 'timber', 'yield', 'status', 'reit', 'make', 'ideal', 'tax', 'deferred', 'account', 'another', 'young', 'timber', 'selection', 'deltic', 'timber', 'corporation', 'del', 'company', 'grows', 'harvest', 'timber', 'acre', 'arkansas', 'louisiana', 'main', 'company', 'goal', 'expand', 'timber', 'holding', 'sustainable', 'harvest', 'level', 'young', 'say', 'share', 'good', 'portfolio', 'counterweight', 'value', 'investor', 'appreciate', 'intrinsic', 'worth', 'underlying', 'real', 'natural', 'resource', 'dick', 'young', 'advice', 'see', 'investment', 'commentary', 'february', 'richard', 'young', 'intelligence', 'report', 'richard', 'young', 'us', 'buy', 'hold', 'strategy', 'mentor', 'warren', 'buffett', 'uncover', 'low', 'risk', 'high', 'reward', 'opportunity', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'inte', 'mtxtu', 'inte', 'undervalued', 'high', 'yield', 'bank', 'put', 'customer', 'first', 'aso', 'amsouth', 'bancorp', 'aso', 'giving', 'investor', 'healthy', 'yield', 'risk', 'involved', 'say', 'jodie', 'wei', 'investment', 'quality', 'trend', 'billion', 'asset', 'amsouth', 'one', 'largest', 'financial', 'institution', 'south', 'office', 'credit', 'bank', 'success', 'putting', 'customer', 'first', 'wei', 'like', 'amsouth', 'us', 'new', 'technology', 'save', 'money', 'streamlining', 'operation', 'note', 'amsouth', 'ranked', 'number', 'six', 'eweek', 'fast', 'track', 'list', 'company', 'deploy', 'cutting', 'edge', 'technology', 'throughout', 'operation', 'number', 'merrill', 'lynch', 'financial', 'service', 'firm', 'placed', 'higher', 'also', 'amsouth', 'internet', 'banking', 'group', 'quadrupled', 'customer', 'base', 'last', 'year', 'wei', 'say', 'aso', 'share', 'undervalued', 'stock', 'selling', 'near', 'yield', 'wei', 'see', 'upside', 'potential', 'dividend', 'risen', 'annually', 'past', 'year', 'buyback', 'plan', 'million', 'share', 'authorized', 'september', 'stock', 'pe', 'reasonable', 'x', 'priced', 'yield', 'aso', 'undervalued', 'buy', 'considered', 'wei', 'say', 'jodie', 'wei', 'advice', 'see', 'investment', 'spotlight', 'january', 'income', 'digest', 'digest', 'excerpt', 'investment', 'publication', 'highlight', 'weather', 'income', 'oriented', 'opportunity', 'uncovered', 'top', 'mind', 'wall', 'street', 'free', 'day', 'trial', 'go', 'investools', 'c', 'go', 'indi', 'mtxtu', 'indi', 'word', 'sponsor', 'new', 'report', 'top', 'pick', 'despite', 'slumping', 'economy', 'shaky', 'stock', 'market', 'frank', 'curzio', 'bull', 'eye', 'pick', 'gained', 'whopping', 'curzio', 'selected', 'stock', 'incredible', 'potential', 'get', 'red', 'hot', 'pick', 'today', 'click', 'investools', 'c', 'go', 'fxcp', 'fxcp', 'mtxtu', 'disclaimer', 'investools', 'advisory', 'published', 'solely', 'informational', 'purpose', 'solicit', 'offer', 'buy', 'sell', 'stock', 'mutual', 'fund', 'security', 'attempt', 'claim', 'complete', 'description', 'security', 'market', 'development', 'referred', 'material', 'expression', 'opinion', 'change', 'without', 'notice', 'information', 'obtained', 'internal', 'external', 'source', 'investools', 'considers', 'reliable', 'investools', 'independently', 'verified', 'information', 'investools', 'guarantee', 'accurate', 'complete', 'investools', 'undertake', 'advise', 'anyone', 'investools', 'employee', 'officer', 'director', 'may', 'time', 'time', 'position', 'security', 'mentioned', 'may', 'sell', 'buy', 'security', 'remove', 'free', 'email', 'list', 'removed', 'email', 'distribution', 'list', 'free', 'investools', 'advisory', 'update', 'simply', 'click', 'link', 'hit', 'send', 'email', 'launched', 'copy', 'paste', 'email', 'address', 'new', 'outgoing', 'email', 'message', 'hit', 'send', 'email', 'launched', 'mailto', 'bonnie', 'investools', 'important', 'automated', 'system', 'cancel', 'paid', 'newsletter', 'service', 'subscription', 'investools', 'tried', 'unsubscribing', 'past', 'believe', 'received', 'message', 'error', 'please', 'send', 'email', 'mailto', 'itfeedback', 'investools', 'voice', 'concern', 'removed', 'list', 'paid', 'subscription', 'information', 'question', 'investools', 'service', 'paid', 'subscription', 'contact', 'investools', 'customer', 'service', 'center', 'investools', 'cgi', 'bin', 'help', 'pl', 'info', 'pr', 'faq', 'html'], ['forwarded', 'richard', 'b', 'sander', 'hou', 'pm', 'justin', 'boyd', 'richard', 'b', 'sander', 'hou', 'mg', 'plc', 'rudolph', 'wolfe', 'richard', 'please', 'see', 'justin', 'forwarded', 'justin', 'boyd', 'lon', 'justin', 'boyd', 'philippa', 'broom', 'glegal', 'enron', 'mg', 'plc', 'rudolph', 'wolfe', 'many', 'thanks', 'enron', 'capital', 'trade', 'resource', 'corp', 'philippa', 'broom', 'glegal', 'justin', 'boyd', 'enron', 'james', 'greig', 'glegal', 'mg', 'plc', 'rudolph', 'wolfe', 'justin', 'uk', 'agreement', 'metallgesellschaft', 'limited', 'mgl', 'rudolf', 'wolff', 'group', 'limited', 'rwg', 'dated', 'june', 'mgl', 'purchased', 'certain', 'lme', 'warrant', 'metal', 'rwg', 'mgl', 'sold', 'certain', 'lme', 'future', 'rwg', 'mgl', 'agreed', 'make', 'certain', 'payment', 'rwg', 'use', 'rwg', 'london', 'hamburg', 'new', 'york', 'office', 'month', 'following', 'closing', 'party', 'agreed', 'process', 'transfer', 'rwg', 'customer', 'mgl', 'mgl', 'agreed', 'make', 'offer', 'employment', 'certain', 'identified', 'employee', 'director', 'rwg', 'group', 'rwg', 'assigned', 'mgl', 'rudolf', 'wolff', 'trademark', 'including', 'company', 'name', 'logo', 'domain', 'name', 'website', 'rwg', 'gave', 'mgl', 'two', 'year', 'non', 'compete', 'covenant', 'u', 'agreement', 'mg', 'metal', 'commodity', 'co', 'limited', 'mgm', 'rudolf', 'wolff', 'metal', 'co', 'inc', 'rwmci', 'dated', 'june', 'mgm', 'purchased', 'certain', 'u', 'non', 'u', 'metal', 'inventory', 'related', 'future', 'contract', 'hedge', 'rwmci', 'rwmci', 'assigned', 'certain', 'forward', 'physical', 'contract', 'mgm', 'mgm', 'agreed', 'make', 'offer', 'employment', 'employee', 'rwmci', 'neither', 'transaction', 'involved', 'sale', 'share', 'rudolf', 'wolff', 'group', 'mg', 'need', 'information', 'please', 'hesitate', 'contact', 'regard', 'philippa', 'philippa', 'broom', 'date', 'justin', 'boyd', 'enron', 'mg', 'plc', 'rudolph', 'wolfe', 'philippa', 'please', 'would', 'prepare', 'brief', 'description', 'underlying', 'rw', 'deal', 'uk', 'u', 'jane', 'allen', 'purpose', 'make', 'short', 'sweet', 'thanks', 'justin', 'forwarded', 'justin', 'boyd', 'lon', 'jeanie', 'slone', 'justin', 'boyd', 'lon', 'madeline', 'fox', 'lon', 'mg', 'plc', 'rudolph', 'wolfe', 'justin', 'please', 'see', 'attached', 'email', 'jane', 'allen', 'houston', 'could', 'provide', 'diagram', 'requested', 'please', 'respond', 'madeline', 'fox', 'thanks', 'best', 'regard', 'jeanie', 'forwarded', 'jeanie', 'slone', 'lon', 'madeline', 'fox', 'jeanie', 'slone', 'lon', 'mg', 'plc', 'rudolph', 'wolfe', 'jeanie', 'want', 'burden', 'last', 'week', 'would', 'easy', 'draw', 'thanks', 'mad', 'forwarded', 'madeline', 'fox', 'lon', 'jane', 'allen', 'hoyt', 'thomas', 'na', 'enron', 'enron', 'madeline', 'fox', 'lon', 'margaret', 'daffin', 'hou', 'melissa', 'laing', 'lon', 'mg', 'plc', 'rudolph', 'wolfe', 'spoken', 'alice', 'gruber', 'immigration', 'attorney', 'order', 'determine', 'get', 'l', 'transferred', 'need', 'basic', 'diagram', 'transaction', 'company', 'example', 'acquired', 'percentage', 'well', 'whether', 'transaction', 'stock', 'based', 'asset', 'acquisition', 'merger', 'please', 'send', 'directly', 'work', 'alice', 'jane', 'j', 'allen', 'enron', 'corp', 'global', 'employee', 'service', 'phone', 'fax', 'forwarded', 'jane', 'allen', 'hou', 'margaret', 'daffin', 'alice', 'gruber', 'agruber', 'tindallfoster', 'enron', 'susan', 'king', 'sking', 'tindallfoster', 'enron', 'jane', 'allen', 'hou', 'melissa', 'laing', 'lon', 'madeline', 'fox', 'lon', 'hoyt', 'thomas', 'na', 'enron', 'enron', 'mg', 'plc', 'rudolph', 'wolfe', 'alice', 'reference', 'following', 'enron', 'recently', 'made', 'asset', 'acquisition', 'rudolf', 'wolf', 'received', 'following', 'note', 'london', 'need', 'help', 'sorting', 'paul', 'rudolf', 'wolf', 'mg', 'owned', 'company', 'paul', 'uk', 'national', 'l', 'visa', 'obtained', 'rudolf', 'wolf', 'company', 'transitioning', 'shortly', 'enron', 'contract', 'sure', 'need', 'immigration', 'perspective', 'transition', 'l', 'visa', 'could', 'let', 'know', 'information', 'lawyer', 'need', 'get', 'ball', 'rolling', 'please', 'let', 'know', 'whether', 'possible', 'bring', 'paul', 'fingster', 'l', 'given', 'fact', 'enron', 'stock', 'transfer', 'many', 'thanks', 'margaret', 'forwarded', 'margaret', 'daffin', 'hou', 'jane', 'allen', 'margaret', 'daffin', 'hou', 'mg', 'plc', 'rudolph', 'wolfe', 'jane', 'j', 'allen', 'enron', 'corp', 'global', 'employee', 'service', 'phone', 'fax', 'forwarded', 'jane', 'allen', 'hou', 'jane', 'allen', 'hoyt', 'thomas', 'na', 'enron', 'enron', 'melissa', 'laing', 'lon', 'enron', 'pm', 'mg', 'plc', 'rudolph', 'wolfe', 'document', 'link', 'jane', 'allen', 'sorry', 'vacation', 'call', 'ingrid', 'wanted', 'see', 'found', 'anything', 'trip', 'ny', 'mg', 'mcc', 'immigration', 'attorney', 'assist', 'visa', 'filing', 'able', 'coach', 'nervous', 'coaching', 'law', 'firm', 'handle', 'visa', 'issue', 'case', 'may', 'different', 'law', 'firm', 'speak', 'individual', 'determine', 'best', 'route', 'take', 'complete', 'paperwork', 'sign', 'jane', 'j', 'allen', 'enron', 'corp', 'global', 'employee', 'service', 'phone', 'fax', 'hoyt', 'thomas', 'enron', 'jane', 'allen', 'hou', 'melissa', 'laing', 'lon', 'mg', 'plc', 'rudolph', 'wolfe', 'pm', 'hi', 'jane', 'talked', 'ingrid', 'krinke', 'today', 'ingrid', 'hr', 'person', 'mg', 'mcc', 'new', 'york', 'branch', 'mg', 'plc', 'mg', 'plc', 'acquired', 'rudolph', 'wolff', 'another', 'british', 'company', 'rudolph', 'wolfe', 'two', 'h', 'one', 'l', 'visa', 'employee', 'turn', 'mg', 'plc', 'asset', 'acquisition', 'rudolph', 'wolff', 'stock', 'transfer', 'ingrid', 'immigration', 'attorney', 'tell', 'cannot', 'transfer', 'l', 'asset', 'transfer', 'ingrid', 'starting', 'process', 'transferring', 'h', 'rw', 'mg', 'know', 'much', 'ingrid', 'know', 'visa', 'would', 'probably', 'good', 'guided', 'something', 'impact', 'u', 'unnecessary', 'ingrid', 'thick', 'file', 'immigration', 'stuff', 'folk', 'know', 'send', 'u', 'could', 'give', 'call', 'talk', 'situation', 'concerned', 'asking', 'right', 'question', 'may', 'garble', 'info', 'pas', 'ingrid', 'told', 'might', 'call', 'closing', 'forecast', 'sept', 'may', 'ny', 'either', 'monday', 'following', 'monday', 'collecting', 'data', 'would', 'happy', 'sit', 'tell', 'know', 'join', 'call', 'ingrid', 'please', 'ring', 'thanks', 'bunch', 'internet', 'email', 'confidentiality', 'footer', 'garretts', 'member', 'andersen', 'worldwide', 'international', 'network', 'law', 'firm', 'list', 'name', 'partner', 'professional', 'qualification', 'open', 'inspection', 'strand', 'london', 'wc', 'r', 'nn', 'principal', 'place', 'business', 'partner', 'either', 'solicitor', 'registered', 'foreign', 'lawyer', 'associate', 'firm', 'scotland', 'dundas', 'wilson', 'privileged', 'confidential', 'information', 'may', 'contained', 'message', 'addressee', 'indicated', 'message', 'responsible', 'delivery', 'message', 'person', 'may', 'copy', 'deliver', 'message', 'anyone', 'case', 'destroy', 'message', 'kindly', 'notify', 'sender', 'reply', 'email', 'please', 'advise', 'immediately', 'employer', 'consent', 'internet', 'email', 'message', 'kind', 'opinion', 'conclusion', 'information', 'message', 'relate', 'official', 'business', 'firm', 'shall', 'understood', 'neither', 'given', 'endorsed'], ['hey', 'wearing', 'target', 'purple', 'shirt', 'today', 'mine', 'want', 'look', 'silly', 'original', 'message', 'baumbach', 'david', 'tuesday', 'october', 'love', 'phillip', 'league', 'due', 'team', 'look', 'good', 'paper', 'come', 'play', 'rc', 'going', 'call', 'original', 'message', 'love', 'phillip', 'tuesday', 'october', 'baumbach', 'david', 'league', 'due', 'much', 'nice', 'machine', 'original', 'message', 'baumbach', 'david', 'tuesday', 'october', 'bass', 'eric', 'ames', 'chuck', 'bucalo', 'harry', 'bucalo', 'harry', 'hull', 'bryan', 'jason', 'bass', 'e', 'mail', 'lenhart', 'matthew', 'love', 'phillip', 'mathews', 'reagan', 'mill', 'bruce', 'rabon', 'chance', 'winfree', 'neal', 'league', 'due', 'way', 'looking', 'gave', 'money', 'away', 'anyway', 'original', 'message', 'bass', 'eric', 'tuesday', 'october', 'ames', 'chuck', 'baumbach', 'david', 'bucalo', 'harry', 'bucalo', 'harry', 'hull', 'bryan', 'jason', 'bass', 'e', 'mail', 'lenhart', 'matthew', 'love', 'phillip', 'mathews', 'reagan', 'mill', 'bruce', 'rabon', 'chance', 'winfree', 'neal', 'league', 'due', 'good', 'news', 'bad', 'news', 'good', 'news', 'invested', 'league', 'due', 'stock', 'market', 'last', 'week', 'bad', 'news', 'invested', 'enron', 'stock', 'needle', 'say', 'going', 'default', 'year', 'payouts', 'sorry', 'guy'], ['leslie', 'milosevich', 'santa', 'clara', 'avenue', 'alameda', 'ca', 'leslie', 'milosevich', 'kp', 'org', 'mr', 'ken', 'lay', 'writing', 'urge', 'donate', 'million', 'dollar', 'made', 'selling', 'enron', 'stock', 'company', 'declared', 'bankruptcy', 'fund', 'enron', 'employee', 'transition', 'fund', 'reach', 'benefit', 'company', 'employee', 'lost', 'retirement', 'saving', 'provide', 'relief', 'low', 'income', 'consumer', 'california', 'afford', 'pay', 'energy', 'bill', 'enron', 'made', 'million', 'pocketbook', 'california', 'consumer', 'effort', 'employee', 'indeed', 'netted', 'well', 'million', 'many', 'enron', 'employee', 'financially', 'devastated', 'company', 'declared', 'bankruptcy', 'retirement', 'plan', 'wiped', 'enron', 'made', 'astronomical', 'profit', 'california', 'energy', 'crisis', 'last', 'year', 'result', 'thousand', 'consumer', 'unable', 'pay', 'basic', 'energy', 'bill', 'largest', 'utility', 'state', 'bankrupt', 'new', 'york', 'time', 'reported', 'sold', 'million', 'worth', 'enron', 'stock', 'aggressively', 'urging', 'company', 'employee', 'keep', 'buying', 'please', 'donate', 'money', 'fund', 'set', 'help', 'repair', 'life', 'american', 'hurt', 'enron', 'underhanded', 'dealing', 'sincerely', 'leslie', 'milosevich'], ['rini', 'twait', 'e', 'th', 'ave', 'longmont', 'co', 'rtwait', 'graphicaljazz', 'mr', 'ken', 'lay', 'writing', 'urge', 'donate', 'million', 'dollar', 'made', 'selling', 'enron', 'stock', 'company', 'declared', 'bankruptcy', 'fund', 'enron', 'employee', 'transition', 'fund', 'reach', 'benefit', 'company', 'employee', 'lost', 'retirement', 'saving', 'provide', 'relief', 'low', 'income', 'consumer', 'california', 'afford', 'pay', 'energy', 'bill', 'enron', 'made', 'million', 'pocketbook', 'california', 'consumer', 'effort', 'employee', 'indeed', 'netted', 'well', 'million', 'many', 'enron', 'employee', 'financially', 'devastated', 'company', 'declared', 'bankruptcy', 'retirement', 'plan', 'wiped', 'enron', 'made', 'astronomical', 'profit', 'california', 'energy', 'crisis', 'last', 'year', 'result', 'thousand', 'consumer', 'unable', 'pay', 'basic', 'energy', 'bill', 'largest', 'utility', 'state', 'bankrupt', 'new', 'york', 'time', 'reported', 'sold', 'million', 'worth', 'enron', 'stock', 'aggressively', 'urging', 'company', 'employee', 'keep', 'buying', 'please', 'donate', 'money', 'fund', 'set', 'help', 'repair', 'life', 'american', 'hurt', 'enron', 'underhanded', 'dealing', 'sincerely', 'rini', 'twait']]\n"
     ]
    }
   ],
   "source": [
    "# Import the lemmatizer from nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Define word cleaning function\n",
    "def clean(text, stop):\n",
    "    text = text.rstrip()\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(i for i in stop_free if i not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(i) for i in punc_free.split())      \n",
    "    return normalized\n",
    "\n",
    "# Clean the emails in df and print results\n",
    "text_clean=[]\n",
    "for text in df['clean_content'][:5]:\n",
    "    if type(text) != str:\n",
    "        continue\n",
    "    text_clean.append(clean(text, stop).split())    \n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned your data entirely with the necessary steps, including splitting the text into words, removing stopwords and punctuations, and lemmatizing your words. You are now ready to run a topic model on this data. In the following exercises you're going to explore how to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic modeling on fraud\n",
    "\n",
    "### Topic modelling: discover hidden patterns in text data\n",
    "1. Discovering topics in text data\n",
    "2. \"What is the text about\"\n",
    "3. Conceptually similar to clustering data\n",
    "4. Compare topics of fraud cases to non-fraud cases and use as a feature or flag\n",
    "5. Or.. is there a particular topic in the data that seems to point to fraud?\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "With LDA you obtain:\n",
    "1. \"topics per text item\" model (i.e. probabilities)\n",
    "2. \"words per topic\" model\n",
    "Creating your own topic model:\n",
    "1. Clean your data\n",
    "2. Create a bag of words with dictionary and corpus\n",
    "3. Feed dictionary and corpus into the LDA model\n",
    "\n",
    "![][30-LDA]\n",
    "\n",
    "### Bag of words: dictionary and corpus\n",
    "```Python\n",
    "from gensim import corpora\n",
    "\n",
    "# Create dictionary number of times a word appears\n",
    "dictionary = corpora.Dictionary(cleaned_emails)\n",
    "\n",
    "# Filter out (non)frequent words\n",
    "dictionary.filter_extremes(no_below=5, keep_n=50000)\n",
    "\n",
    "# Create corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in cleaned_emails]\n",
    "```\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA) with gensim\n",
    "```Python\n",
    "import gensim\n",
    "\n",
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 3,\n",
    "id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the three topics from the model with top words\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "print(topic)\n",
    "(0, 0.029*email + 0.016*send + 0.016*results + 0.016*invoice)\n",
    "(1, 0.026*price + 0.026*work + 0.026*management + 0.026*sell)\n",
    "(2, 0.029*distribute + 0.029*contact + 0.016*supply + 0.016*fast)\n",
    "```\n",
    "\n",
    "# To learn more in detail about LDA see:\n",
    "- DATACAMP: [Latent Semantic Analysis using Python][2]\n",
    "- Towards Data Science: [Topic Modeling in Python: Latent Dirichlet Allocation (LDA)][1]\n",
    "- Machine Learning Plus: [Topic Modeling with Gensim (Python)][3]\n",
    "\n",
    "[30-LDA]:_Docs/30-LDA.png\n",
    "[1]: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "[2]: https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
    "[3]: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create dictionary and corpus\n",
    "In order to run an LDA topic model, you first need to **define your dictionary and corpus** first, as those need to go into the model. You're going to continue working on the cleaned text data that you've done in the previous exercises. That means that `text_clean` is available for you already to continue working with, and you'll use that to create your dictionary and corpus.\n",
    "\n",
    "This exercise will take a little longer to execute than usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 6), (6, 1), (7, 2), (8, 4), (9, 1), (10, 1), (11, 3), (12, 2), (13, 1), (14, 5), (15, 3), (16, 1), (17, 3), (18, 1), (19, 1), (20, 1), (21, 5), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 3), (31, 3), (32, 1), (33, 3), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 2), (42, 1), (43, 1), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 4), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 4), (56, 1), (57, 4), (58, 9), (59, 5), (60, 1), (61, 8), (62, 1), (63, 1), (64, 2), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 2), (78, 1), (79, 1), (80, 1), (81, 12), (82, 2), (83, 2), (84, 1), (85, 1), (86, 3), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1), (95, 4), (96, 1), (97, 1), (98, 2), (99, 2), (100, 1), (101, 2), (102, 5), (103, 1), (104, 3), (105, 8), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 2), (117, 4), (118, 1), (119, 1), (120, 1), (121, 1), (122, 1), (123, 1), (124, 1), (125, 3), (126, 1), (127, 1), (128, 1), (129, 1), (130, 2), (131, 2), (132, 2), (133, 1), (134, 1), (135, 7), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 1), (152, 1), (153, 3), (154, 1), (155, 1), (156, 4), (157, 1), (158, 2), (159, 5), (160, 3), (161, 1), (162, 2), (163, 2), (164, 1), (165, 10), (166, 9), (167, 3), (168, 1), (169, 2), (170, 1), (171, 8), (172, 1), (173, 8), (174, 1), (175, 1), (176, 2), (177, 1), (178, 13), (179, 2), (180, 1), (181, 2), (182, 1), (183, 1), (184, 2), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 2), (191, 2), (192, 1), (193, 5), (194, 1), (195, 1), (196, 1), (197, 1), (198, 3), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 3), (208, 5), (209, 2), (210, 2), (211, 1), (212, 1), (213, 1), (214, 1), (215, 6), (216, 2), (217, 1), (218, 1), (219, 1), (220, 1), (221, 3), (222, 1), (223, 1), (224, 2), (225, 1), (226, 1), (227, 1), (228, 1), (229, 1), (230, 1), (231, 1), (232, 1), (233, 1), (234, 2), (235, 8), (236, 23), (237, 4), (238, 1), (239, 1), (240, 1), (241, 5), (242, 2), (243, 1), (244, 1), (245, 3), (246, 1), (247, 1), (248, 1), (249, 3), (250, 4), (251, 3), (252, 2), (253, 4), (254, 1), (255, 4), (256, 1), (257, 1), (258, 3), (259, 1), (260, 5), (261, 2), (262, 1), (263, 1), (264, 2), (265, 1), (266, 1), (267, 1), (268, 1), (269, 2), (270, 1), (271, 1), (272, 1), (273, 3), (274, 3), (275, 1), (276, 1), (277, 1), (278, 9), (279, 1), (280, 1), (281, 2), (282, 2), (283, 2), (284, 1), (285, 1), (286, 1), (287, 2), (288, 1), (289, 10), (290, 1), (291, 1), (292, 1), (293, 3), (294, 3), (295, 2), (296, 1), (297, 1), (298, 7), (299, 2), (300, 1), (301, 1), (302, 1), (303, 1), (304, 4), (305, 1), (306, 4), (307, 2), (308, 1), (309, 2), (310, 1), (311, 2), (312, 1), (313, 1), (314, 1), (315, 1), (316, 3), (317, 1), (318, 2), (319, 1), (320, 4), (321, 1), (322, 1), (323, 1), (324, 1), (325, 1), (326, 1), (327, 2), (328, 1), (329, 3), (330, 1), (331, 2), (332, 1), (333, 3), (334, 2), (335, 1), (336, 1), (337, 1), (338, 5), (339, 2), (340, 1), (341, 1), (342, 2), (343, 1), (344, 2), (345, 1), (346, 2), (347, 1), (348, 1), (349, 2), (350, 2), (351, 1), (352, 1), (353, 2), (354, 1), (355, 1), (356, 1), (357, 1), (358, 1), (359, 1), (360, 1), (361, 1), (362, 1), (363, 1), (364, 1), (365, 1), (366, 1), (367, 1), (368, 4), (369, 1), (370, 1), (371, 1), (372, 1), (373, 1), (374, 2), (375, 1), (376, 2), (377, 1), (378, 1), (379, 1), (380, 1), (381, 2), (382, 1), (383, 1), (384, 1), (385, 1), (386, 1), (387, 1), (388, 1), (389, 1), (390, 1), (391, 2), (392, 3), (393, 1), (394, 1), (395, 1), (396, 2), (397, 1), (398, 1), (399, 2), (400, 2), (401, 1), (402, 1), (403, 1), (404, 1), (405, 2), (406, 5), (407, 2), (408, 1), (409, 6), (410, 1), (411, 2), (412, 1), (413, 1), (414, 1), (415, 9), (416, 3), (417, 2), (418, 2), (419, 12), (420, 1), (421, 1), (422, 4), (423, 12), (424, 1), (425, 1), (426, 2), (427, 5), (428, 2), (429, 3), (430, 1), (431, 1), (432, 6), (433, 1), (434, 1), (435, 7), (436, 1), (437, 1), (438, 1), (439, 1), (440, 1), (441, 1), (442, 1), (443, 1), (444, 1), (445, 1), (446, 3), (447, 1), (448, 1), (449, 1), (450, 1), (451, 1), (452, 2), (453, 1), (454, 1), (455, 1), (456, 1), (457, 2), (458, 14), (459, 1), (460, 1), (461, 1), (462, 4), (463, 1), (464, 1), (465, 3), (466, 1), (467, 1), (468, 1), (469, 1), (470, 1), (471, 4), (472, 1), (473, 1), (474, 1), (475, 3), (476, 1), (477, 1), (478, 1), (479, 1), (480, 2), (481, 2), (482, 1), (483, 10), (484, 1), (485, 2), (486, 2), (487, 3), (488, 1), (489, 1), (490, 3), (491, 1), (492, 6), (493, 1), (494, 1), (495, 1), (496, 1), (497, 2), (498, 1), (499, 1), (500, 1), (501, 1), (502, 6), (503, 1), (504, 1), (505, 1), (506, 1), (507, 2), (508, 1), (509, 1), (510, 1), (511, 1), (512, 1), (513, 1), (514, 6), (515, 1), (516, 1), (517, 1), (518, 1), (519, 1), (520, 1), (521, 1), (522, 6), (523, 1), (524, 1), (525, 2), (526, 1), (527, 1), (528, 12), (529, 8), (530, 9), (531, 3), (532, 1), (533, 1)], [(6, 1), (17, 1), (18, 1), (23, 4), (42, 2), (57, 2), (66, 1), (81, 5), (82, 1), (89, 1), (93, 1), (102, 1), (113, 1), (121, 1), (135, 4), (136, 6), (158, 5), (162, 1), (168, 2), (176, 2), (181, 1), (183, 3), (192, 1), (220, 1), (221, 4), (231, 2), (238, 1), (239, 1), (247, 6), (250, 1), (259, 1), (260, 1), (273, 4), (277, 2), (282, 5), (287, 7), (294, 1), (305, 1), (306, 2), (309, 1), (312, 1), (313, 2), (314, 1), (316, 1), (317, 1), (319, 1), (337, 3), (344, 9), (372, 1), (380, 1), (390, 1), (402, 1), (409, 3), (416, 1), (423, 3), (429, 2), (432, 3), (435, 1), (436, 1), (458, 3), (469, 1), (472, 1), (486, 1), (497, 6), (500, 1), (528, 1), (534, 1), (535, 2), (536, 3), (537, 1), (538, 4), (539, 2), (540, 1), (541, 4), (542, 13), (543, 1), (544, 1), (545, 1), (546, 2), (547, 1), (548, 1), (549, 1), (550, 3), (551, 2), (552, 1), (553, 1), (554, 1), (555, 7), (556, 1), (557, 1), (558, 1), (559, 1), (560, 3), (561, 1), (562, 1), (563, 4), (564, 1), (565, 2), (566, 6), (567, 2), (568, 2), (569, 1), (570, 1), (571, 1), (572, 1), (573, 1), (574, 1), (575, 1), (576, 1), (577, 1), (578, 1), (579, 1), (580, 3), (581, 4), (582, 3), (583, 1), (584, 4), (585, 1), (586, 1), (587, 2), (588, 1), (589, 1), (590, 1), (591, 1), (592, 2), (593, 2), (594, 1), (595, 1), (596, 1), (597, 1), (598, 1), (599, 1), (600, 1), (601, 2), (602, 1), (603, 2), (604, 1), (605, 20), (606, 1), (607, 1), (608, 3), (609, 1), (610, 1), (611, 1), (612, 1), (613, 4), (614, 1), (615, 1), (616, 1), (617, 8), (618, 1), (619, 6), (620, 1), (621, 1), (622, 1), (623, 1), (624, 2), (625, 3), (626, 3), (627, 1), (628, 2), (629, 1), (630, 2), (631, 1), (632, 1), (633, 1), (634, 1), (635, 1), (636, 1), (637, 9), (638, 1), (639, 4), (640, 1), (641, 1), (642, 1), (643, 5), (644, 1), (645, 1), (646, 1), (647, 1), (648, 1), (649, 9), (650, 1), (651, 1), (652, 1), (653, 3), (654, 1), (655, 14), (656, 5), (657, 1), (658, 2), (659, 11), (660, 1), (661, 1), (662, 1), (663, 1), (664, 6), (665, 4), (666, 3), (667, 2), (668, 2), (669, 3), (670, 2), (671, 1), (672, 13), (673, 3), (674, 1), (675, 1), (676, 6), (677, 5), (678, 2), (679, 4), (680, 1), (681, 1), (682, 4), (683, 1), (684, 20), (685, 8), (686, 4), (687, 1), (688, 2), (689, 1), (690, 3), (691, 3), (692, 1), (693, 5), (694, 2), (695, 1), (696, 1), (697, 2), (698, 1), (699, 2), (700, 1), (701, 1), (702, 1), (703, 1), (704, 2), (705, 1), (706, 1), (707, 3), (708, 1), (709, 1), (710, 2), (711, 1), (712, 5), (713, 1), (714, 1), (715, 14), (716, 3), (717, 1), (718, 1), (719, 1), (720, 1), (721, 1), (722, 2), (723, 1), (724, 1), (725, 2), (726, 1), (727, 1), (728, 1), (729, 1), (730, 2), (731, 1), (732, 1), (733, 1), (734, 1), (735, 1), (736, 1), (737, 1), (738, 1), (739, 1), (740, 1), (741, 1), (742, 7), (743, 14), (744, 2), (745, 9), (746, 4), (747, 2), (748, 1), (749, 1), (750, 1), (751, 1), (752, 1), (753, 1), (754, 1), (755, 1), (756, 1), (757, 3), (758, 1), (759, 1), (760, 1), (761, 1), (762, 1), (763, 1), (764, 1), (765, 1), (766, 1), (767, 1), (768, 1), (769, 1), (770, 1), (771, 1), (772, 2), (773, 6), (774, 1), (775, 4), (776, 2), (777, 1), (778, 1), (779, 1), (780, 3), (781, 5), (782, 1), (783, 1), (784, 1), (785, 1), (786, 1), (787, 1), (788, 2), (789, 3), (790, 1), (791, 1), (792, 1), (793, 1), (794, 6), (795, 1), (796, 1), (797, 1), (798, 1), (799, 1), (800, 1), (801, 1), (802, 2), (803, 1), (804, 3), (805, 12), (806, 6), (807, 1), (808, 1), (809, 4), (810, 2)], [(104, 4), (180, 2), (181, 3), (250, 1), (268, 1), (278, 1), (287, 4), (293, 1), (343, 1), (419, 1), (458, 2), (473, 1), (486, 1), (528, 1), (563, 1), (605, 1), (622, 1), (689, 1), (761, 1), (795, 1), (800, 1), (811, 2), (812, 1), (813, 1), (814, 2), (815, 4), (816, 4), (817, 2), (818, 2), (819, 4), (820, 2), (821, 2), (822, 1), (823, 1), (824, 5), (825, 2), (826, 2), (827, 1), (828, 4), (829, 1), (830, 2), (831, 2), (832, 2), (833, 5), (834, 2), (835, 2), (836, 1), (837, 4), (838, 2), (839, 2), (840, 2), (841, 2), (842, 1), (843, 2), (844, 1), (845, 4), (846, 1), (847, 4), (848, 4), (849, 1), (850, 1), (851, 4), (852, 1), (853, 2), (854, 1), (855, 2), (856, 1), (857, 1), (858, 1), (859, 4), (860, 1), (861, 1), (862, 2)], [(35, 2), (81, 4), (107, 2), (136, 5), (167, 3), (192, 1), (208, 1), (249, 1), (250, 1), (266, 1), (277, 1), (289, 4), (293, 1), (306, 1), (342, 1), (344, 1), (364, 1), (386, 1), (404, 1), (428, 1), (458, 2), (485, 1), (526, 1), (528, 1), (554, 1), (605, 7), (675, 3), (724, 1), (758, 1), (784, 1), (801, 1), (810, 1), (863, 1), (864, 1), (865, 1), (866, 1), (867, 1), (868, 1), (869, 1), (870, 1), (871, 2), (872, 1), (873, 1), (874, 3), (875, 1), (876, 3), (877, 1), (878, 1), (879, 1), (880, 1), (881, 2), (882, 1), (883, 3), (884, 1), (885, 1), (886, 1), (887, 1), (888, 1), (889, 1), (890, 1), (891, 3), (892, 1), (893, 1), (894, 3), (895, 1), (896, 1), (897, 1), (898, 2), (899, 1), (900, 1), (901, 1), (902, 1), (903, 2), (904, 1), (905, 1), (906, 1), (907, 1), (908, 1), (909, 1), (910, 1), (911, 1), (912, 1), (913, 1), (914, 1), (915, 1), (916, 1)], [(35, 2), (81, 4), (107, 2), (136, 5), (167, 3), (192, 1), (208, 1), (249, 1), (250, 1), (266, 1), (277, 1), (289, 4), (293, 1), (306, 1), (342, 1), (344, 1), (364, 1), (386, 1), (404, 1), (428, 1), (458, 2), (485, 1), (526, 1), (528, 1), (554, 1), (568, 1), (605, 7), (675, 3), (724, 1), (758, 1), (784, 1), (801, 1), (810, 1), (825, 1), (863, 1), (864, 1), (866, 1), (867, 1), (869, 1), (870, 1), (871, 2), (872, 1), (874, 3), (876, 3), (877, 1), (878, 1), (879, 1), (880, 1), (881, 2), (882, 1), (883, 3), (884, 1), (885, 1), (886, 1), (887, 1), (888, 1), (890, 1), (892, 1), (893, 1), (895, 1), (896, 1), (898, 2), (899, 1), (900, 1), (901, 1), (902, 1), (903, 2), (905, 1), (906, 1), (907, 1), (908, 1), (909, 1), (910, 1), (911, 1), (912, 1), (913, 1), (914, 1), (915, 1), (916, 1), (917, 1), (918, 1), (919, 1), (920, 2), (921, 1), (922, 1), (923, 2)]]\n",
      "Dictionary(924 unique tokens: ['account', 'accurate', 'acquiring', 'acre', 'address']...)\n"
     ]
    }
   ],
   "source": [
    "# Import the packages\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Define the dictionary\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "\n",
    "# Define the corpus \n",
    "corpus = [dictionary.doc2bow(text) for text in text_clean]\n",
    "\n",
    "# Print corpus and dictionary\n",
    "print(corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the two ingredients you need to run your topic model on the enron emails. You are now ready for the final step and create your first fraud detection topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LDA model\n",
    "Now it's time to build the **LDA model**. Using the `dictionary` and `corpus`, you are ready to discover which topics are present in the Enron emails. With a quick print of words assigned to the topics, you can do a first exploration about whether there are any obvious topics that jump out. Be mindful that the topic model is **heavy to calculate** so it will take a while to run. Let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.015*\"league\" + 0.015*\"due\" + 0.012*\"message\" + 0.012*\"love\" + 0.012*\"phillip\"')\n",
      "(1, '0.010*\"investools\" + 0.006*\"year\" + 0.006*\"stock\" + 0.006*\"free\" + 0.006*\"go\"')\n",
      "(2, '0.021*\"enron\" + 0.021*\"mg\" + 0.015*\"plc\" + 0.015*\"rudolph\" + 0.015*\"jane\"')\n",
      "(3, '0.031*\"enron\" + 0.022*\"employee\" + 0.019*\"company\" + 0.018*\"million\" + 0.014*\"fund\"')\n",
      "(4, '0.018*\"investools\" + 0.011*\"stock\" + 0.010*\"go\" + 0.010*\"see\" + 0.010*\"company\"')\n"
     ]
    }
   ],
   "source": [
    "# Define the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=5)\n",
    "\n",
    "# Save the topics and top 5 words\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "\n",
    "# Print the results\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now successfully created your first topic model on the Enron email data. However, the print of words doesn't really give you enough information to find a topic that might lead you to signs of fraud. You'll therefore need to closely inspect the model results in order to be able to detect anything that can be related to fraud in your data. You'll learn more about this in the next video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flagging fraud based on topics\n",
    "### Using your LDA model results for fraud detection\n",
    "1. Are there any suspicious topics? (no labels)\n",
    "2. Are the topics in fraud and non-fraud cases similar? (with labels)\n",
    "3. Are fraud cases associated more with certain topics? (with labels)\n",
    "\n",
    "### To understand topics, you need to visualize\n",
    "\n",
    "- pyLDAvis.gensim only works in jupyter notebooks\n",
    "\n",
    "```Python\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(ldamodel, corpus,\n",
    "dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)\n",
    "```\n",
    "\n",
    "### Inspecting how topics differ\n",
    "\n",
    "![][31-topics]\n",
    "\n",
    "### Assign topics to your original data\n",
    "```Python\n",
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_details_df = topic_details_df.append(pd.Series([topic_num,\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score']\n",
    "    return topic_details_df\n",
    "                                                                      \n",
    "contents = pd.DataFrame({'Original text':text_clean})\n",
    "topic_details = pd.concat([get_topic_details(ldamodel,corpus), contents], axis=1)\n",
    "topic_details.head()\n",
    "                                                                      \n",
    "Dominant_Topic % Score Original text\n",
    "0 0.0 0.989108 [investools, advisory, free, ...\n",
    "1 0.0 0.993513 [forwarded, richard, b, ...\n",
    "2 1.0 0.964858 [hey, wearing, target, purple, ...\n",
    "3 0.0 0.989241 [leslie, milosevich, santa, clara, ...\n",
    "```\n",
    "[31-topics]:_Docs/31-topics.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Interpreting the topic model\n",
    "Below are visualisation results from the pyLDAvis library available. Have a look at topic 1 and 3 from the LDA model on the Enron email data. Which one would you research further for fraud detection purposes and why?\n",
    "\n",
    "- Topic 1:\n",
    "![][32-pyLDAvis_topic1]\n",
    "\n",
    "- Topic 3\n",
    "![][33-pyLDAvis_topic3]\n",
    "\n",
    "**Possible Answers**\n",
    "- [x] Topic 1.\n",
    "> **Correct:** Topic 1 seems to discuss the employee share option program, and seems to point to internal conversation (with \"please, may, know\" etc), so this is more likely to be related to the internal accounting fraud and trading stock with insider knowledge. *Topic 3 seems to be more related to general news around Enron.*\n",
    "- [ ] Topic 3.\n",
    "- [ ] None of these topics seem related to fraud.\n",
    "\n",
    "[32-pyLDAvis_topic1]:_Docs/32-pyLDAvis_topic1.png\n",
    "[33-pyLDAvis_topic3]:_Docs/33-pyLDAvis_topic3.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finding fraudsters based on topic\n",
    "In this exercise you're going to **link the results** from the topic model **back to your original data**. You now learned that you want to **flag** everything related to **topic 3**. As you will see, this is actually not that straightforward. You'll be given the function `get_topic_details()` which takes the arguments `ldamodel` and `corpus`. It retrieves the details of the topics for each line of text. With that function, you can append the results back to your original data. If you want to learn more detail on how to work with the model results, which is beyond the scope of this course, you're highly encouraged to read this article: [Topic Modeling with Gensim (Python)][1].\n",
    "\n",
    "Available for you are the `dictionary` and `corpus`, the text data `text_clean` as well as your model results `ldamodel`. Also defined is `get_topic_details()`.\n",
    "\n",
    "[1]:https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_df = pd.DataFrame()\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0: # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = ', '.join(str(topic[0]) for topic in wp)\n",
    "                topic_details_df = topic_details_df.append(pd.Series([topic_num,prop_topic, topic_keywords]), ignore_index=True)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score', 'Topic_Keywords']\n",
    "    return topic_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Dominant_Topic   % Score                                     Topic_Keywords\n",
      "0             4.0  0.999229  investools, stock, go, see, company, say, year...\n",
      "1             2.0  0.998953  enron, mg, plc, rudolph, jane, lon, allen, wol...\n",
      "2             0.0  0.994390  league, due, message, love, phillip, david, bu...\n",
      "3             3.0  0.993535  enron, employee, company, million, fund, consu...\n",
      "4             3.0  0.993427  enron, employee, company, million, fund, consu...\n"
     ]
    }
   ],
   "source": [
    "# Run get_topic_details function and check the results\n",
    "print(get_topic_details(ldamodel, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Original text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>investools, stock, go, see, company, say, year...</td>\n",
       "      <td>[investools, advisory, free, digest, trusted, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>enron, mg, plc, rudolph, jane, lon, allen, wol...</td>\n",
       "      <td>[forwarded, richard, b, sander, hou, pm, justi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>league, due, message, love, phillip, david, bu...</td>\n",
       "      <td>[hey, wearing, target, purple, shirt, today, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993535</td>\n",
       "      <td>enron, employee, company, million, fund, consu...</td>\n",
       "      <td>[leslie, milosevich, santa, clara, avenue, ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993427</td>\n",
       "      <td>enron, employee, company, million, fund, consu...</td>\n",
       "      <td>[rini, twait, e, th, ave, longmont, co, rtwait...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic   % Score  \\\n",
       "0             4.0  0.999229   \n",
       "1             2.0  0.998953   \n",
       "2             0.0  0.994390   \n",
       "3             3.0  0.993535   \n",
       "4             3.0  0.993427   \n",
       "\n",
       "                                      Topic_Keywords  \\\n",
       "0  investools, stock, go, see, company, say, year...   \n",
       "1  enron, mg, plc, rudolph, jane, lon, allen, wol...   \n",
       "2  league, due, message, love, phillip, david, bu...   \n",
       "3  enron, employee, company, million, fund, consu...   \n",
       "4  enron, employee, company, million, fund, consu...   \n",
       "\n",
       "                                       Original text  \n",
       "0  [investools, advisory, free, digest, trusted, ...  \n",
       "1  [forwarded, richard, b, sander, hou, pm, justi...  \n",
       "2  [hey, wearing, target, purple, shirt, today, m...  \n",
       "3  [leslie, milosevich, santa, clara, avenue, ala...  \n",
       "4  [rini, twait, e, th, ave, longmont, co, rtwait...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add original text to topic details in a dataframe\n",
    "contents = pd.DataFrame({'Original text': text_clean})\n",
    "topic_details = pd.concat([get_topic_details(ldamodel, corpus), contents], axis=1)\n",
    "topic_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>% Score</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Original text</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.999229</td>\n",
       "      <td>investools, stock, go, see, company, say, year...</td>\n",
       "      <td>[investools, advisory, free, digest, trusted, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>enron, mg, plc, rudolph, jane, lon, allen, wol...</td>\n",
       "      <td>[forwarded, richard, b, sander, hou, pm, justi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994390</td>\n",
       "      <td>league, due, message, love, phillip, david, bu...</td>\n",
       "      <td>[hey, wearing, target, purple, shirt, today, m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993535</td>\n",
       "      <td>enron, employee, company, million, fund, consu...</td>\n",
       "      <td>[leslie, milosevich, santa, clara, avenue, ala...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.993427</td>\n",
       "      <td>enron, employee, company, million, fund, consu...</td>\n",
       "      <td>[rini, twait, e, th, ave, longmont, co, rtwait...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dominant_Topic   % Score  \\\n",
       "0             4.0  0.999229   \n",
       "1             2.0  0.998953   \n",
       "2             0.0  0.994390   \n",
       "3             3.0  0.993535   \n",
       "4             3.0  0.993427   \n",
       "\n",
       "                                      Topic_Keywords  \\\n",
       "0  investools, stock, go, see, company, say, year...   \n",
       "1  enron, mg, plc, rudolph, jane, lon, allen, wol...   \n",
       "2  league, due, message, love, phillip, david, bu...   \n",
       "3  enron, employee, company, million, fund, consu...   \n",
       "4  enron, employee, company, million, fund, consu...   \n",
       "\n",
       "                                       Original text  flag  \n",
       "0  [investools, advisory, free, digest, trusted, ...     0  \n",
       "1  [forwarded, richard, b, sander, hou, pm, justi...     0  \n",
       "2  [hey, wearing, target, purple, shirt, today, m...     0  \n",
       "3  [leslie, milosevich, santa, clara, avenue, ala...     1  \n",
       "4  [rini, twait, e, th, ave, longmont, co, rtwait...     1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create flag for text highest associated with topic 3\n",
    "topic_details['flag'] = np.where((topic_details['Dominant_Topic'] == 3.0), 1, 0)\n",
    "topic_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You have now flagged all data that is highest associated with topic 3, that seems to cover internal conversation about enron stock options**. You are a true detective. With these exercises you have demonstrated that text mining and topic modeling can be a powerful tool for fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fraud detection in Python Recap\n",
    "\n",
    "### 1. Working with imbalanced data\n",
    "- Worked with highly imbalanced fraud data\n",
    "- Learned how to resample your data\n",
    "- Learned about different resampling methods\n",
    "\n",
    "### 2. Fraud detection with labeled data\n",
    "- Refreshed supervised learning techniques to detect fraud\n",
    "- Learned how to get reliable performance metrics and worked with the precision recall trade-off\n",
    "- Explored how to optimise your model parameters to handle fraud data\n",
    "- Applied ensemble methods to fraud detection\n",
    "\n",
    "### 3. Fraud detection without labels\n",
    "- Learned about the importance of segmentation\n",
    "- Refreshed your knowledge on clustering methods\n",
    "- Learned how to detect fraud using outliers and small clusters with K-means clustering\n",
    "- Applied a DB-scan clustering model for fraud detection\n",
    "\n",
    "### 4. Text mining for fraud detection\n",
    "- Know how to augment fraud detection analysis with text mining techniques\n",
    "- Applied word searches to flag use of certain words, and learned how to apply topic modelling for fraud detection\n",
    "- Learned how to effectively clean messy text data\n",
    "\n",
    "### 5. Further learning for fraud detection\n",
    "- Network analysis to detect fraud\n",
    "- Different supervised and unsupervised learning techniques (e.g. Neural Networks)\n",
    "- Working with very large data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
